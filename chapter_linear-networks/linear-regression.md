# 선형 회귀
:label:`sec_linear_regression`

*회귀*는 모델링을 위한 일련의 방법을 나타냅니다.
하나 이상의 독립 변수와 종속 변수 간의 관계.자연 과학 및 사회 과학에서 회귀의 목적은 가장 자주
*입력과 출력 간의 관계를 특성화* 합니다.
반면 기계 학습은*예측*과 가장 관련이 있습니다. 

수치 값을 예측하고 싶을 때마다 회귀 문제가 나타납니다.일반적인 예로는 가격 예측 (주택, 주식 등), 체류 기간 예측 (병원 환자의 경우), 수요 예측 (소매 판매) 등이 있습니다.모든 예측 문제가 전형적인 회귀 문제는 아닙니다.다음 섹션에서는 분류 문제를 소개합니다. 여기서 목표는 일련의 범주 간의 멤버십을 예측하는 것입니다. 

## 선형 회귀의 기본 요소

*선형 회귀*는 둘 다 가장 단순할 수 있습니다
회귀에 대한 표준 도구 중에서 가장 많이 사용됩니다.19세기의 새벽으로 거슬러 올라가는 선형 회귀는 몇 가지 간단한 가정에서 비롯됩니다.먼저 독립 변수 $\mathbf{x}$와 종속 변수 $y$ 간의 관계가 선형이라고 가정합니다. 즉, 관측치에 약간의 잡음이 주어지면 $y$를 $\mathbf{x}$의 요소의 가중 합으로 표현할 수 있습니다.둘째, 모든 노이즈가 잘 동작한다고 가정합니다 (가우스 분포에 따라). 

접근 방식에 동기를 부여하기 위해 실행 예제부터 시작하겠습니다.면적 (평방 피트) 과 연령 (년) 을 기준으로 주택 가격 (달러) 을 추정한다고 가정합니다.주택 가격을 예측하는 모델을 실제로 개발하려면 각 주택의 판매 가격, 면적 및 연령을 알고있는 판매로 구성된 데이터 세트를 손에 넣어야합니다.기계 학습 용어에서 데이터 세트는*훈련 데이터 세트* 또는*훈련 세트*라고하며 각 행 (여기서는 하나의 판매에 해당하는 데이터) 을*예제* (또는*데이터 포인트*, *데이터 인스턴스*, *샘플*) 라고합니다.우리가 예측하려는 것 (가격) 을*라벨* (또는*목표*) 라고합니다.예측의 기반이 되는 독립 변수 (연령 및 면적) 를*특징* (또는*공변량*) 라고 합니다. 

일반적으로 데이터셋의 예제 수를 나타내기 위해 $n$을 사용합니다.각 입력을 $\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top$로 나타내고 해당 레이블을 $y^{(i)}$로 나타내는 $i$로 데이터 예제를 색인화합니다. 

### 선형 모델
:label:`subsec_linear_model`

선형성 가정은 목표 (가격) 가 특징 (면적 및 연령) 의 가중 합계로 표현 될 수 있다고 말합니다. 

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.$$
:eqlabel:`eq_price-area`

:eqref:`eq_price-area`에서는 $w_{\mathrm{area}}$ 및 $w_{\mathrm{age}}$을 *가중치*라고 하고 $b$를 *편향* (*오프셋* 또는*가로채기*라고도 함) 라고 합니다.가중치는 각 피처가 예측에 미치는 영향을 결정하며 편향은 모든 피처가 값 0을 취할 때 예측 가격이 취해야 하는 값을 나타냅니다.면적이 0이거나 정확히 0 년 된 집을 볼 수 없더라도 여전히 편견이 필요합니다. 그렇지 않으면 모델의 표현력을 제한 할 것입니다.엄밀히 말하면 :eqref:`eq_price-area`는 입력 특성의*아핀 변환*으로, 가중 합을 통한 특징의*선형 변환*과 추가된 바이어스를 통해*변환*과 결합되는 특징이 있습니다. 

데이터셋이 주어지면 평균적으로 모델에 따른 예측이 데이터에서 관찰된 실제 가격에 가장 적합하도록 가중치 $\mathbf{w}$와 편향 $b$를 선택하는 것이 목표입니다.입력 특징의 아핀 변환에 의해 출력 예측이 결정되는 모델은*선형 모델*이며, 여기서 아핀 변환은 선택한 가중치와 치우침으로 지정됩니다. 

몇 가지 특징만 있는 데이터셋에 초점을 맞추는 것이 일반적인 분야에서는 이와 같이 모델을 긴 형식으로 명시적으로 표현하는 것이 일반적입니다.기계 학습에서는 일반적으로 고차원 데이터 세트로 작업하므로 선형 대수 표기법을 사용하는 것이 더 편리합니다.입력이 $d$ 특징으로 구성된 경우 예측 $\hat{y}$ (일반적으로 “모자”기호는 추정치를 나타냄) 를 다음과 같이 표현합니다. 

$$\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$

모든 특징을 벡터 $\mathbf{x} \in \mathbb{R}^d$로 수집하고 모든 가중치를 벡터 $\mathbf{w} \in \mathbb{R}^d$로 수집하면 내적을 사용하여 모델을 콤팩트하게 표현할 수 있습니다. 

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
:eqlabel:`eq_linreg-y`

:eqref:`eq_linreg-y`에서 벡터 $\mathbf{x}$은 단일 데이터 예제의 특징에 해당합니다.*디자인 매트릭스* $\mathbf{X} \in \mathbb{R}^{n \times d}$을 통해 $n$ 예제로 구성된 전체 데이터 세트의 기능을 참조하는 것이 편리할 때가 많습니다.여기서 $\mathbf{X}$에는 모든 예제에 대해 하나의 행과 모든 피처에 대해 하나의 열이 포함되어 있습니다. 

특징 모음 $\mathbf{X}$의 경우 예측 $\hat{\mathbf{y}} \in \mathbb{R}^n$는 행렬-벡터 곱을 통해 표현할 수 있습니다. 

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

여기서 합산 중에 브로드캐스트 (:numref:`subsec_broadcasting` 참조) 가 적용됩니다.훈련 데이터셋 $\mathbf{X}$ 및 해당 (알려진) 레이블 $\mathbf{y}$의 특징이 주어지면 선형 회귀의 목적은 $\mathbf{X}$과 동일한 분포에서 샘플링된 새 데이터 예제의 특징이 주어진 가중치 벡터 $\mathbf{w}$ 및 편향 항 $b$를 찾는 것입니다.기대) 를 가장 낮은 오차로 예측합니다. 

$\mathbf{x}$이 주어진 $y$을 예측하는 데 가장 적합한 모델이 선형이라고 생각하더라도 $n$ 예제의 실제 데이터 세트는 찾을 것으로 기대하지 않을 것입니다. 여기서 $y^{(i)}$은 모든 $1 \leq i \leq n$에 대해 $\mathbf{w}^\top \mathbf{x}^{(i)}+b$과 정확히 같습니다.예를 들어, 특징 $\mathbf{X}$ 및 레이블 $\mathbf{y}$를 관찰하기 위해 사용하는 기기는 소량의 측정 오류가 발생할 수 있습니다.따라서 기본 관계가 선형이라고 확신하는 경우에도 이러한 오류를 설명하기 위해 잡음 항을 통합할 것입니다. 

최상의 매개 변수* (또는*모델 매개 변수*) $\mathbf{w}$ 및 $b$를 검색하기 전에 (i) 특정 모델에 대한 품질 측정 및 (ii) 품질을 개선하기 위해 모델을 업데이트하는 절차라는 두 가지가 더 필요합니다. 

### 손실 함수

모델에 데이터를*피팅하는 방법에 대해 생각하기 전에*적합성*의 척도를 결정해야 합니다.*손실 함수*는 목표값의*실제*와*예측된* 값 사이의 거리를 수량화합니다.손실은 일반적으로 음수가 아닌 숫자이며, 값이 작을수록 더 좋고 완벽한 예측은 0의 손실을 초래합니다.회귀 문제에서 가장 많이 사용되는 손실 함수는 제곱 오차입니다.예제 $i$에 대한 예측이 $\hat{y}^{(i)}$이고 해당하는 실제 레이블이 $y^{(i)}$인 경우 제곱 오차는 다음과 같이 계산됩니다. 

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
:eqlabel:`eq_mse`

상수 $\frac{1}{2}$는 실질적인 차이를 만들지 않지만 표기적으로 편리하다는 것을 증명하여 손실의 파생물을 취할 때 취소됩니다.훈련 데이터 세트가 우리에게 제공되어 통제 할 수 없기 때문에 경험적 오차는 모델 매개 변수의 함수 일뿐입니다.좀 더 구체적으로 만들려면 :numref:`fig_fit_linreg`와 같이 1차원 사례에 대한 회귀 문제를 그리는 아래 예를 고려하십시오. 

![Fit data with a linear model.](../img/fit-linreg.svg)
:label:`fig_fit_linreg`

추정치 $\hat{y}^{(i)}$과 관측치 $y^{(i)}$ 간의 차이가 크면 2차 의존성으로 인해 손실에 더 큰 기여를 하게 됩니다.$n$ 예제의 전체 데이터 세트에서 모델의 품질을 측정하기 위해 훈련 세트의 손실을 평균 (또는 동등하게 합계) 합니다. 

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

모델을 훈련시킬 때 모든 훈련 예제에서 총 손실을 최소화하는 매개 변수 ($\mathbf{w}^*, b^*$) 를 찾으려고 합니다. 

$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$

### 분석 솔루션

선형 회귀는 비정상적으로 단순한 최적화 문제입니다.이 책에서 접하게 될 대부분의 다른 모델과 달리 선형 회귀는 간단한 공식을 적용하여 분석적으로 풀 수 있습니다.먼저 모든 열로 구성된 설계 행렬에 열을 추가하여 치우침 $b$를 모수 $\mathbf{w}$으로 가정할 수 있습니다.그렇다면 우리의 예측 문제는 $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$를 최소화하는 것입니다.손실 표면에는 단 하나의 임계점이 있으며 이는 전체 도메인에 대한 최소 손실에 해당합니다.$\mathbf{w}$에 대한 손실의 미분을 취하여 0으로 설정하면 분석적 (폐쇄형) 해가 생성됩니다. 

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.$$

선형 회귀와 같은 간단한 문제가 분석적 해를 인정할 수 있지만 이러한 행운에 익숙해서는 안됩니다.분석 솔루션은 훌륭한 수학적 분석을 허용하지만 분석 솔루션의 요구 사항은 너무 제한적이어서 모든 딥 러닝을 제외할 수 있습니다. 

### 미니배치 확률적 경사하강법

모델을 분석적으로 풀 수 없는 경우에도 실제로 모델을 효과적으로 훈련시킬 수 있다는 것이 밝혀졌습니다.더욱이 많은 작업에서 최적화하기 어려운 모델은 훨씬 더 나은 것으로 판명되어 훈련 방법을 알아내는 것이 결국 문제의 가치가 있습니다. 

거의 모든 딥러닝 모델을 최적화하기 위한 핵심 기법이자 이 책에서 다룰 내용은 손실 함수를 점진적으로 낮추는 방향으로 파라미터를 업데이트하여 오류를 반복적으로 줄이는 것입니다.이 알고리즘을*기울기 하강*이라고 합니다. 

기울기 하강법의 가장 순진한 적용은 손실 함수의 도함수를 취하는 것으로 구성되며, 이는 데이터셋의 모든 단일 예에서 계산된 손실의 평균입니다.실제로는 속도가 매우 느릴 수 있습니다. 단일 업데이트를 수행하기 전에 전체 데이터세트를 넘겨야 합니다.따라서 업데이트를 계산해야 할 때마다 임의의 미니 배치 예제를 샘플링하는 경우가 많습니다. 이 변형은*minibatch 확률적 기울기 하강*입니다. 

각 반복에서 먼저 고정된 수의 훈련 예제로 구성된 미니배치 $\mathcal{B}$를 랜덤하게 샘플링합니다.그런 다음 모델 매개 변수와 관련하여 미니 배치에서 평균 손실의 미분 (기울기) 을 계산합니다.마지막으로 기울기에 미리 결정된 양수 값 $\eta$를 곱하고 결과 항을 현재 매개 변수 값에서 뺍니다. 

업데이트를 다음과 같이 수학적으로 표현할 수 있습니다 ($\partial$는 편미분을 나타냄). 

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

요약하면 알고리즘의 단계는 다음과 같습니다. (i) 일반적으로 무작위로 모델 매개 변수의 값을 초기화합니다. (ii) 데이터에서 무작위 미니 배치를 반복적으로 샘플링하여 음의 기울기 방향으로 매개 변수를 업데이트합니다.2차 손실 및 아핀 변환의 경우 다음과 같이 명시적으로 작성할 수 있습니다. 

$$\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}$$
:eqlabel:`eq_linreg_batch_update`

$\mathbf{w}$와 $\mathbf{x}$은 :eqref:`eq_linreg_batch_update`의 벡터입니다.여기서 더 우아한 벡터 표기법은 $w_1, w_2, \ldots, w_d$와 같은 계수로 표현하는 것보다 수학을 훨씬 더 읽기 쉽게 만듭니다.설정된 카디널리티 $|\mathcal{B}|$은 각 미니배치의 예제 수 (*배치 크기*) 를 나타내고 $\eta$은*학습률*을 나타냅니다.배치 크기 및 학습률의 값은 수동으로 사전 지정되며 일반적으로 모델 학습을 통해 학습되지 않는다는 점을 강조합니다.조정 가능하지만 훈련 루프에서 업데이트되지 않는 이러한 파라미터를*하이퍼파라미터*라고 합니다.
*하이퍼파라미터 튜닝*은 하이퍼파라미터가 선택되는 프로세스입니다.
일반적으로 별도의*검증 데이터세트* (또는*검증 세트*) 에서 평가된 훈련 루프의 결과에 따라 조정해야 합니다. 

미리 결정된 반복 횟수에 대해 훈련 한 후 (또는 다른 중지 기준이 충족 될 때까지) $\hat{\mathbf{w}}, \hat{b}$로 표시된 추정 모델 매개 변수를 기록합니다.함수가 실제로 선형적이고 잡음이 없더라도 알고리즘이 최소화를 향해 천천히 수렴하더라도 한정된 수의 단계로는 정확하게 달성 할 수 없기 때문에 이러한 매개 변수가 손실의 정확한 최소화가 아닙니다. 

선형 회귀는 전체 영역에서 최솟값이 하나뿐인 학습 문제입니다.그러나 심층 네트워크와 같이 더 복잡한 모델의 경우 손실 표면에는 많은 최소값이 포함됩니다.다행스럽게도 아직 완전히 이해되지 않은 이유로 딥 러닝 실무자는 훈련 세트에서* 손실을 최소화하는 매개 변수를 찾는 데 거의 어려움을 겪지 않습니다.더 강력한 작업은*일반화*라는 도전 과제인 이전에 보지 못했던 데이터 손실을 줄일 수 있는 매개 변수를 찾는 것입니다.책 전체에서 이러한 주제로 돌아갑니다. 

### 학습된 모델을 사용한 예측

학습된 선형 회귀 모델 $\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$이 주어지면 이제 면적 $x_1$와 $x_2$세가 주어지면 새 주택의 가격 (훈련 데이터에 포함되지 않음) 을 추정할 수 있습니다.특징이 주어진 목표값을 추정하는 것을 일반적으로*예측* 또는*추론*이라고 합니다. 

딥 러닝에서 표준 전문 용어로 부상하고 있음에도 불구하고 이 단계를*추론*이라고 부르는 것은 다소 잘못된 이름이기 때문에*예측*을 고수하려고 노력할 것입니다.통계에서*추론*은 데이터셋을 기반으로 모수를 추정하는 것을 나타내는 경우가 더 많습니다.이러한 용어의 오용은 딥러닝 실무자가 통계학자와 대화할 때 흔히 혼란을 야기합니다. 

## 속도를 위한 벡터화

모델을 학습시킬 때 일반적으로 예제의 전체 미니 배치를 동시에 처리하기를 원합니다.이를 효율적으로 수행하려면 (**we**) (~~should~~) (**계산을 벡터화하고 파이썬에서 값 비싼 for 루프를 작성하는 대신 빠른 선형 대수 라이브러리를 활용하십시오.**)

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
import math
import numpy as np
import time
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import numpy as np
import time
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
import numpy as np
import time
```

이것이 왜 그렇게 중요한지 설명하기 위해 (**벡터를 추가하는 두 가지 방법을 고려하십시오.**) 시작하기 위해 모든 벡터를 포함하는 두 개의 1000차원 벡터를 인스턴스화합니다.한 가지 방법에서는 Python for 루프를 사용하여 벡터를 반복합니다.다른 방법에서는 `+`에 대한 단일 호출에 의존합니다.

```{.python .input}
#@tab all
n = 10000
a = d2l.ones(n)
b = d2l.ones(n)
```

이 책에서는 실행 시간을 자주 벤치마킹하므로 [**타이머를 정의하겠습니다**].

```{.python .input}
#@tab all
class Timer:  #@save
    """Record multiple running times."""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """Start the timer."""
        self.tik = time.time()

    def stop(self):
        """Stop the timer and record the time in a list."""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """Return the average time."""
        return sum(self.times) / len(self.times)

    def sum(self):
        """Return the sum of time."""
        return sum(self.times)

    def cumsum(self):
        """Return the accumulated time."""
        return np.array(self.times).cumsum().tolist()
```

이제 워크로드를 벤치마킹할 수 있습니다.먼저, [**for 루프를 사용하여 한 번에 하나의 좌표씩 추가합니다.**]

```{.python .input}
#@tab mxnet, pytorch
c = d2l.zeros(n)
timer = Timer()
for i in range(n):
    c[i] = a[i] + b[i]
f'{timer.stop():.5f} sec'
```

```{.python .input}
#@tab tensorflow
c = tf.Variable(d2l.zeros(n))
timer = Timer()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'{timer.stop():.5f} sec'
```

(**또는 다시 로드된 `+` 연산자를 사용하여 요소별 합계를 계산합니다.**)

```{.python .input}
#@tab all
timer.start()
d = a + b
f'{timer.stop():.5f} sec'
```

두 번째 방법이 첫 번째 방법보다 훨씬 빠르다는 것을 알았을 것입니다.코드를 벡터화하면 크기가 훨씬 빨라지는 경우가 많습니다.또한 더 많은 수학을 라이브러리로 푸시하고 많은 계산을 직접 작성할 필요가 없으므로 오류 가능성이 줄어 듭니다. 

## 정규 분포와 손실 제곱
:label:`subsec_normal_distribution_and_squared_loss`

위의 정보만을 사용하여 이미 손을 더럽힐 수 있지만 다음에서는 소음 분포에 대한 가정을 통해 제곱 손실 목표에 더 공식적으로 동기를 부여 할 수 있습니다. 

선형 회귀는 1795년 가우스에 의해 발명되었으며, 정규 분포 (*가우시안*라고도 함) 도 발견했습니다.정규 분포와 선형 회귀 사이의 연결은 공통 혈통보다 더 깊다는 것이 밝혀졌습니다.기억을 새로 고치기 위해 평균이 $\mu$이고 분산이 $\sigma^2$ (표준 편차 $\sigma$) 인 정규 분포의 확률 밀도는 다음과 같이 지정됩니다. 

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

아래 [**정규 분포를 계산하는 Python 함수를 정의합니다**].

```{.python .input}
#@tab all
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

이제 (**정규 분포를 시각화**) 할 수 있습니다.

```{.python .input}
#@tab all
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

보시다시피 평균을 변경하면 $x$ 축을 따라 이동하는 것과 일치하고 분산을 늘리면 분포가 확산되어 피크가 낮아집니다. 

평균 제곱 오차 손실 함수 (또는 단순히 손실 제곱) 를 사용하여 선형 회귀를 유도하는 한 가지 방법은 잡음이 있는 관측값이 다음과 같이 정규 분포를 따르는 잡음이 있는 관측값에서 발생한다고 공식적으로 가정하는 것입니다. 

$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).$$

따라서 이제 주어진 $\mathbf{x}$에 대해 특정 $y$를 볼 가능성*을 쓸 수 있습니다. 

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$

이제 최대우도 원칙에 따라 모수 $\mathbf{w}$ 및 $b$의 최적 값은 전체 데이터셋의*가능성*을 최대화하는 값입니다. 

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$

최대우도 원칙에 따라 선택된 추정기를*최대우도 추정기*라고 합니다.많은 지수 함수의 곱을 최대화하는 것은 어려워 보일 수 있지만 목적을 변경하지 않고 우도의 로그를 최대화하여 작업을 크게 단순화 할 수 있습니다.과거의 이유로 최적화는 최대화보다는 최소화로 표현되는 경우가 더 많습니다.따라서 아무것도 변경하지 않고*음의 로그 가능도* $-\log P(\mathbf y \mid \mathbf X)$를 최소화할 수 있습니다.수학 학습은 다음과 같은 이점을 제공합니다. 

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$

이제 $\sigma$이 고정 상수라는 가정이 하나 더 필요합니다.따라서 첫 번째 용어는 $\mathbf{w}$ 또는 $b$에 의존하지 않기 때문에 무시할 수 있습니다.이제 두 번째 항은 승법 상수 $\frac{1}{\sigma^2}$를 제외하고 이전에 소개된 오차 손실 제곱과 동일합니다.다행히도 솔루션은 $\sigma$에 의존하지 않습니다.따라서 평균 제곱 오차를 최소화하는 것은 가산성 가우스 잡음을 가정하고 선형 모델의 최대우도 추정과 같습니다. 

## 선형 회귀에서 심층 네트워크로

지금까지 선형 모델에 대해서만 이야기했습니다.신경망은 훨씬 더 풍부한 모델 제품군을 포괄하지만 선형 모델을 신경망 언어로 표현하여 신경망으로 생각할 수 있습니다.먼저 “계층” 표기법으로 다시 작성해 보겠습니다. 

### 신경망 다이어그램

딥 러닝 실무자들은 모델에서 일어나는 일을 시각화하기 위해 다이어그램을 그리는 것을 좋아합니다.:numref:`fig_single_neuron`에서는 선형 회귀 모델을 신경망으로 묘사합니다.이 다이어그램은 각 입력이 출력에 연결되는 방식과 같은 연결 패턴을 강조하지만 가중치나 편향에 의해 취해진 값은 강조 표시하지 않습니다. 

![Linear regression is a single-layer neural network.](../img/singleneuron.svg)
:label:`fig_single_neuron`

:numref:`fig_single_neuron`에 표시된 신경망의 경우 입력값은 $x_1, \ldots, x_d$이므로 입력 계층의*입력 수* (또는*특징 차원성*) 는 $d$입니다.:numref:`fig_single_neuron`의 네트워크 출력은 $o_1$이므로 출력 계층의*출력 수*는 1입니다.입력 값은 모두*주어진*이며, *계산된* 뉴런은 하나뿐입니다.계산이 이루어지는 위치에 초점을 맞추면서 일반적으로 레이어를 계산할 때 입력 레이어를 고려하지 않습니다.즉, :numref:`fig_single_neuron`에서 신경망에 대한*레이어 수*는 1입니다.선형 회귀 모델은 단일 인공 뉴런으로 구성된 신경망 또는 단일 계층 신경망으로 생각할 수 있습니다. 

선형 회귀의 경우 모든 입력이 모든 출력에 연결되므로 (이 경우 출력은 하나만 있음) 이 변환 (:numref:`fig_single_neuron`의 출력 계층) 을*완전 연결 계층* 또는* 조밀 계층*으로 간주 할 수 있습니다.다음 장에서는 이러한 계층으로 구성된 네트워크에 대해 더 자세히 설명하겠습니다. 

### 생물학

선형 회귀 (1795년에 발명 됨) 가 전산 신경 과학보다 우선하기 때문에 선형 회귀를 신경망으로 설명하는 것은 시대 착오적으로 보일 수 있습니다.사이버네틱스 학자/신경 생리 학자 워렌 맥 컬록 (Warren McCulloch) 과 월터 피츠 (Walter Pitts) 가 인공 뉴런 모델을 개발하기 시작했을 때 선형 모델이 시작하기에 자연스러운 장소였던 이유를 알아 보려면 73229-3614 년에 다음과 같이 구성된 생물학적 뉴런의 만화 같은 그림을 고려하십시오.
*수상 돌기* (입력 단자),
*핵* (CPU), *축삭* (출력 와이어) 및*축삭 터미널* (출력 단자) 을 사용하여*시냅스*를 통해 다른 뉴런에 연결할 수 있습니다. 

![The real neuron.](../img/neuron.svg)
:label:`fig_Neuron`

다른 뉴런 (또는 망막과 같은 환경 센서) 에서 도착하는 정보 $x_i$이 수상 돌기에서 수신됩니다.특히, 이 정보는 입력의 효과 (예를 들어, 생성물 $x_i w_i$를 통한 활성화 또는 억제) 를 결정하는*시냅스 중량* $w_i$에 의해 가중된다.다수의 소스로부터 도착하는 가중 입력은 가중 합 ($y = \sum_i x_i w_i + b$) 으로서 핵에서 집계되고, 이 정보는 전형적으로 $\sigma(y)$를 통한 일부 비선형 처리 후에 축삭 (axon $y$) 에서 추가 처리를 위해 전송된다.거기에서 목적지 (예: 근육) 에 도달하거나 수상 돌기를 통해 다른 뉴런으로 공급됩니다. 

확실히, 그러한 많은 단위들이 올바른 연결성 및 올바른 학습 알고리즘과 함께 자갈길 수 있다는 높은 수준의 생각은 어떤 뉴런만으로도 표현할 수있는 것보다 훨씬 더 흥미롭고 복잡한 행동을 생성 할 수 있다는 것은 실제 생물학적 신경계에 대한 우리의 연구에 빚지고 있습니다. 

동시에 오늘날 딥 러닝에 대한 대부분의 연구는 신경 과학에서 직접적인 영감을 거의 얻지 못합니다.우리는 고전적인 AI 교과서에서 스튜어트 러셀과 피터 노비그를 호출합니다.
*인공 지능 : A Modern Approach* :cite:`Russell.Norvig.2016`,
비행기가 조류로부터 영감을 받았을지도 모르지만 조류학은 수세기 동안 항공 혁신의 주요 동인이 아니라고 지적했습니다.마찬가지로 요즘 딥 러닝에서 영감을 얻은 것은 수학, 통계 및 컴퓨터 과학에서 동등하거나 더 큰 척도로 나타납니다. 

## 요약

* 머신 러닝 모델의 핵심 요소는 학습 데이터, 손실 함수, 최적화 알고리즘, 그리고 모델 자체입니다.
* 벡터화는 모든 것을 더 좋게 (대부분 수학) 하고 더 빠르게 (대부분 코드) 만듭니다.
* 목적 함수를 최소화하고 최대우도 추정을 수행하는 것도 같은 의미일 수 있습니다.
* 선형 회귀 모델도 신경망입니다.

## 연습문제

1. $x_1, \ldots, x_n \in \mathbb{R}$의 일부 데이터가 있다고 가정합니다.우리의 목표는 $\sum_i (x_i - b)^2$가 최소화되도록 상수 $b$를 찾는 것입니다.
    1. 최적값 $b$에 대한 분석적 해를 구합니다.
    1. 이 문제와 그 해는 정규 분포와 어떤 관련이 있습니까?
1. 오차 제곱이 있는 선형 회귀에 대한 최적화 문제에 대한 해석적 해를 도출합니다.단순하게 유지하기 위해 문제에서 편향 $b$를 생략할 수 있습니다 (모든 열로 구성된 $\mathbf X$에 하나의 열을 추가하여 원칙적인 방식으로 이 작업을 수행할 수 있습니다).
    1. 행렬과 벡터 표기법으로 최적화 문제를 작성합니다. 모든 데이터를 단일 행렬로 처리하고 모든 대상 값을 단일 벡터로 처리합니다.
    1. $w$에 대한 손실의 기울기를 계산합니다.
    1. 기울기를 0으로 설정하고 행렬 방정식을 풀어 해석적 해를 구합니다.
    1. 확률적 경사하강법을 사용하는 것보다 언제 더 좋을까요?이 방법은 언제 깨질 수 있을까요?
1. 가산 잡음 $\epsilon$를 제어하는 잡음 모델이 지수 분포라고 가정합니다.즉, $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$입니다.
    1. 모형 $-\log P(\mathbf y \mid \mathbf X)$에서 데이터의 음의 로그 우도를 기록합니다.
    1. 비공개 양식 솔루션을 찾을 수 있습니까?
    1. 이 문제를 해결하기 위해 확률적 경사하강법 알고리즘을 제안합니다.무엇이 잘못 될 수 있습니까 (힌트: 매개 변수를 계속 업데이트하면서 고정 지점 근처에서 어떤 일이 발생합니까)?이 문제를 해결할 수 있을까요?

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/40)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/258)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/259)
:end_tab:
