# 소프트맥스 회귀
:label:`sec_softmax`

:numref:`sec_linear_regression`에서는 선형 회귀를 도입하여 :numref:`sec_linear_scratch`에서 처음부터 구현을 수행하고 :numref:`sec_linear_concise`에서 딥 러닝 프레임워크의 상위 수준 API를 사용하여 무거운 작업을 수행했습니다. 

회귀는 우리가 대답하고 싶을 때 도달하는 망치입니다.*얼마나?* 또는*몇 개입니까?* 질문.주택을 판매 할 달러 (가격) 수, 야구 팀의 승리 수 또는 환자가 퇴원하기 전에 입원 할 일수를 예측하려면 회귀 모델을 찾고있을 것입니다. 

실제로 우리는*분류*에 더 관심이 있습니다. “얼마나”가 아니라 “어느 것”을 묻습니다. 

* 이 이메일이 스팸 폴더 또는 받은 편지함에 속해 있습니까?
* 이 고객이 구독 서비스에 가입할 가능성이 높습니까? 또는*가입하지 않음*
* 이 이미지는 당나귀, 개, 고양이 또는 수탉을 묘사합니까?
* Aston이 다음에 볼 가능성이 가장 높은 영화는 무엇입니까?

구어체로 기계 학습 실무자는*분류*라는 단어를 과부하하여 미묘하게 다른 두 가지 문제를 설명합니다. (i) 범주 (클래스) 에 대한 예제를 어려운 할당에만 관심이있는 문제; (ii) 소프트 과제를 수행하려는 경우, 즉각 카테고리가 적용됩니다.부분적으로는 구별이 흐려지는 경향이 있습니다. 어려운 과제에만 관심이 있더라도 소프트 할당을 수행하는 모델을 여전히 사용하기 때문입니다. 

## 분류 문제
:label:`subsec_classification-problem`

발을 젖게 하기 위해 간단한 이미지 분류 문제부터 시작해 보겠습니다.여기서 각 입력은 $2\times2$ 회색조 영상으로 구성됩니다.각 픽셀 값을 단일 스칼라로 표현할 수 있으며, $x_1, x_2, x_3, x_4$의 네 가지 특징을 제공합니다.또한 각 이미지가 “고양이”, “닭”및 “개”범주 중 하나에 속한다고 가정합니다. 

다음으로 레이블을 나타내는 방법을 선택해야 합니다.우리에게는 두 가지 분명한 선택이 있습니다.아마도 가장 자연스러운 충동은 $y \in \{1, 2, 3\}$를 선택하는 것입니다. 여기서 정수는 각각 $\{\text{dog}, \text{cat}, \text{chicken}\}$를 나타냅니다.이러한 정보를 컴퓨터에 저장하는*좋은 방법입니다.카테고리들 사이에 자연스러운 순서가 있는 경우 (예: $\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$을 예측하려는 경우) 이 문제를 회귀로 캐스팅하고 레이블을 이 형식으로 유지하는 것이 합리적일 수 있습니다. 

그러나 일반적인 분류 문제는 수업 간의 자연 순서와 함께 발생하지 않습니다.다행히 통계학자들은 오래 전에 범주형 데이터를 나타내는 간단한 방법인 *one-hot 인코딩*을 발명했습니다.원-핫 인코딩은 카테고리 수만큼의 구성 요소를 가진 벡터입니다.특정 인스턴스의 범주에 해당하는 구성 요소는 1로 설정되고 다른 모든 구성 요소는 0으로 설정됩니다.우리의 경우 레이블 $y$은 3 차원 벡터이며 $(1, 0, 0)$은 “고양이”에 해당하고 $(0, 1, 0)$는 “닭”에 해당하고 $(0, 0, 1)$는 “개”에 해당합니다. 

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

## 네트워크 아키텍처

가능한 모든 클래스와 관련된 조건부 확률을 추정하려면 클래스당 하나씩 출력값이 여러 개인 모델이 필요합니다.선형 모델을 사용하여 분류를 처리하려면 출력값만큼 많은 아핀 함수가 필요합니다.각 출력은 자체 아핀 함수에 해당합니다.이 경우 4 개의 특징과 3 개의 가능한 출력 범주가 있으므로 가중치를 나타 내기 위해 12 개의 스칼라 (아래 첨자가있는 $w$) 와 편향을 나타내려면 3 개의 스칼라 (아래 첨자가있는 $b$) 가 필요합니다.각 입력에 대해 다음 세 가지*로짓*, $o_1, o_2$ 및 $o_3$를 계산합니다. 

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

:numref:`fig_softmaxreg`에 표시된 신경망 다이어그램으로 이 계산을 묘사할 수 있습니다.선형 회귀와 마찬가지로 소프트맥스 회귀도 단일 계층 신경망입니다.또한 각 출력값 $o_1, o_2$ 및 $o_3$의 계산은 모든 입력 ($x_1$, $x_2$, $x_3$ 및 $x_4$) 에 따라 달라지므로 소프트맥스 회귀의 출력 계층도 완전 연결 계층으로 설명할 수 있습니다. 

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

모델을 더 간결하게 표현하기 위해 선형 대수 표기법을 사용할 수 있습니다.벡터 형식에서는 수학과 코드 작성에 더 적합한 양식 인 $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$에 도달합니다.모든 가중치를 $3 \times 4$ 행렬로 수집했으며 주어진 데이터 예제 $\mathbf{x}$의 특징에 대해 출력은 입력 특성과 편향 $\mathbf{b}$에 의해 가중치의 행렬-벡터 곱으로 제공됩니다. 

## 완전 연결 계층의 파라미터화 비용
:label:`subsec_parameterization-cost-fc-layers`

다음 장에서 볼 수 있듯이 완전 연결 계층은 딥 러닝에서 어디에나 있습니다.그러나 이름에서 알 수 있듯이 완전 연결 계층은 잠재적으로 많은 학습 가능한 파라미터와*완전히* 연결됩니다.특히, $d$ 입력 및 $q$ 출력이 있는 완전 연결 계층의 경우 파라미터화 비용은 $\mathcal{O}(dq)$이며, 이는 실제로 엄청나게 높을 수 있습니다.다행스럽게도 $d$ 입력을 $q$ 출력으로 변환하는 이러한 비용을 $\mathcal{O}(\frac{dq}{n})$로 줄일 수 있습니다. 여기서 하이퍼 파라미터 $n$는 실제 응용 프로그램 :cite:`Zhang.Tay.Zhang.ea.2021`에서 매개 변수 저장과 모델 효율성 간의 균형을 맞추기 위해 유연하게 지정할 수 있습니다. 

## 소프트맥스 운영
:label:`subsec_softmax_operation`

여기서 취할 주요 접근법은 모델의 출력을 확률로 해석하는 것입니다.관측된 데이터의 가능성을 최대화하는 확률을 생성하기 위해 모수를 최적화할 것입니다.그런 다음 예측을 생성하기 위해 임계 값을 설정합니다. 예를 들어 예측 확률이 최대 인 레이블을 선택합니다. 

공식적으로 말하면, 모든 출력 $\hat{y}_j$를 주어진 항목이 클래스 $j$에 속할 확률로 해석되기를 바랍니다.그런 다음 출력 값이 가장 큰 클래스를 예측 $\operatorname*{argmax}_j y_j$로 선택할 수 있습니다.예를 들어 $\hat{y}_1$, $\hat{y}_2$ 및 $\hat{y}_3$이 각각 0.1, 0.8 및 0.1인 경우 (이 예에서는) “닭”을 나타내는 카테고리 2를 예측합니다. 

로짓 $o$를 관심 출력으로 직접 해석하도록 제안하고 싶을 수도 있습니다.그러나 선형 계층의 출력값을 확률로 직접 해석하는 데는 몇 가지 문제가 있습니다.한편으로는 이 숫자의 합이 1로 제한되는 것은 없습니다.반면에 입력에 따라 음수 값을 취할 수 있습니다.이는 :numref:`sec_prob`에 제시된 확률의 기본 공리를 위반합니다. 

출력값을 확률로 해석하려면 (새 데이터에서도) 음이 아니고 합이 최대 1이 되도록 보장해야 합니다.또한 모델이 충실하게 확률을 추정하도록 장려하는 훈련 목표가 필요합니다.분류기가 0.5를 출력하는 모든 인스턴스 중에서 이러한 예제 중 절반이 실제로 예측된 클래스에 속하기를 바랍니다.이것은*교정*이라는 속성입니다. 

1959년 사회과학자 R. Duncan Luce가 *초이스 모델*이라는 맥락에서 발명한*소프트맥스 기능*이 바로 이를 수행합니다.로짓이 음이 아니고 합이 1이되도록 변환하고 모델을 미분 가능한 상태로 유지하도록 요구하려면 먼저 각 로짓을 지수 화 (비 음수를 보장) 한 다음 합으로 나눕니다 (합이 1이 되도록 보장). 

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{where}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}. $$
:eqlabel:`eq_softmax_y_and_o`

모든 $j$에 대해 $0 \leq \hat{y}_j \leq 1$과 함께 $\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$을 쉽게 볼 수 있습니다.따라서 $\hat{\mathbf{y}}$는 요소 값을 적절히 해석할 수 있는 적절한 확률 분포입니다.소프트맥스 연산은 단순히 각 클래스에 할당된 확률을 결정하는 소프트맥스 이전 값인 로짓 $\mathbf{o}$ 간의 순서를 변경하지 않습니다.따라서 예측 중에 다음과 같이 가장 가능성이 높은 클래스를 선택할 수 있습니다. 

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

소프트맥스는 비선형 함수이지만 소프트맥스 회귀의 출력값은 여전히 입력 특징의 아핀 변환에 의해*결정됩니다*. 따라서 소프트맥스 회귀는 선형 모델입니다. 

## 미니배치의 벡터화
:label:`subsec_softmax_vectorization`

계산 효율성을 개선하고 GPU를 활용하기 위해 일반적으로 데이터의 미니 배치에 대한 벡터 계산을 수행합니다.피쳐 차원 (입력 수) $d$ 및 배치 크기 $n$가 있는 예제의 미니배치 $\mathbf{X}$이 제공된다고 가정합니다.또한 출력에 $q$개의 범주가 있다고 가정합니다.그런 다음 미니배치 피쳐 $\mathbf{X}$이 $\mathbb{R}^{n \times d}$에 있고 가중치는 $\mathbf{W} \in \mathbb{R}^{d \times q}$이며 편향은 $\mathbf{b} \in \mathbb{R}^{1\times q}$를 충족합니다. 

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

이것은 한 번에 하나의 예제를 처리하면 실행될 행렬-벡터 곱과 비교하여 행렬-행렬 곱 $\mathbf{X} \mathbf{W}$로의 지배적 연산을 가속화합니다.$\mathbf{X}$의 각 행은 데이터 예제를 나타내므로 소프트맥스 연산 자체를*rowwise*로 계산할 수 있습니다. $\mathbf{O}$의 각 행에 대해 모든 항목을 지수화한 다음 합계로 정규화합니다.:eqref:`eq_minibatch_softmax_reg`에서 합계 $\mathbf{X} \mathbf{W} + \mathbf{b}$ 동안 브로드캐스트를 트리거하는 경우, 미니배치 로짓 $\mathbf{O}$과 출력 확률 $\hat{\mathbf{Y}}$는 모두 $n \times q$ 행렬입니다. 

## 손실 함수

다음으로 예측된 확률의 품질을 측정하기 위해 손실 함수가 필요합니다.선형 회귀 (:numref:`subsec_normal_distribution_and_squared_loss`) 에서 평균 제곱 오차 목적에 대한 확률적 정당성을 제공할 때 접했던 것과 동일한 개념인 최대 우도 추정에 의존할 것입니다. 

### 로그 우도

소프트맥스 함수는 벡터 $\hat{\mathbf{y}}$을 제공하며, 입력 $\mathbf{x}$ (예: $\hat{y}_1$ = $P(y=\text{cat} \mid \mathbf{x})$) 이 주어지면 각 클래스의 추정 조건부 확률로 해석할 수 있습니다.전체 데이터셋 $\{\mathbf{X}, \mathbf{Y}\}$에 $n$의 예가 있다고 가정합니다. 여기서 $i$로 인덱싱된 예제는 특징 벡터 $\mathbf{x}^{(i)}$과 원핫 레이블 벡터 $\mathbf{y}^{(i)}$로 구성됩니다.다음과 같은 특징이 주어지면 실제 클래스가 모델에 따라 얼마나 가능한지 확인하여 추정치를 현실과 비교할 수 있습니다. 

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

최대우도 추정에 따르면 $P(\mathbf{Y} \mid \mathbf{X})$를 최대화합니다. 이는 음의 로그 우도를 최소화하는 것과 같습니다. 

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

여기서 $q$ 클래스에 대한 레이블 $\mathbf{y}$ 및 모델 예측 $\hat{\mathbf{y}}$의 모든 쌍에 대해 손실 함수 $l$은 다음과 같습니다. 

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

나중에 설명하는 이유로 :eqref:`eq_l_cross_entropy`의 손실 함수를 일반적으로*교차 엔트로피 손실*이라고합니다.$\mathbf{y}$은 길이가 $q$인 원핫 벡터이므로 모든 좌표 $j$에 대한 합은 한 항을 제외한 모든 항에서 사라집니다.모든 $\hat{y}_j$는 예측 확률이므로 로그는 $0$보다 크지 않습니다.결과적으로*확실성*으로 실제 레이블을 올바르게 예측하면 손실 함수를 더 이상 최소화 할 수 없습니다. 즉, 실제 레이블 $\mathbf{y}$에 대한 예측 확률 $P(\mathbf{y} \mid \mathbf{x}) = 1$이면 손실 함수를 더 이상 최소화 할 수 없습니다.이는 종종 불가능하다는 점에 유의하십시오.예를 들어 데이터셋에 레이블 노이즈가 있을 수 있습니다 (일부 예에서는 레이블이 잘못 지정될 수 있음).입력 기능이 모든 예제를 완벽하게 분류하기에 충분한 정보가 없는 경우에도 가능하지 않을 수 있습니다. 

### 소프트맥스 및 파생 상품
:label:`subsec_softmax_and_derivatives`

소프트맥스와 그에 상응하는 손실은 매우 흔하기 때문에 계산 방법을 조금 더 잘 이해할 가치가 있습니다.:eqref:`eq_softmax_y_and_o`를 :eqref:`eq_l_cross_entropy`의 손실 정의에 연결하고 소프트맥스의 정의를 사용하여 다음과 같이 얻습니다. 

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

무슨 일이 일어나고 있는지 좀 더 잘 이해하려면 로짓 $o_j$에 대한 도함수를 고려하십시오.우리는 얻는다 

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

즉, 미분은 소프트맥스 연산으로 표현되는 모델에 의해 할당된 확률과 원핫 레이블 벡터의 요소로 표현된 실제 발생한 확률 간의 차이입니다.이런 의미에서 기울기는 관측치 $y$와 추정치 $\hat{y}$의 차이인 회귀 분석에서 본 것과 매우 유사합니다.이것은 우연이 아닙니다.지수 집합 ([online appendix on distributions](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html) 참조) 모형에서 로그 우도의 기울기는 정확하게 이 항으로 지정됩니다.따라서 실제로 그래디언트를 쉽게 계산할 수 있습니다. 

### 교차 엔트로피 손실

이제 단일 결과뿐만 아니라 결과에 대한 전체 분포를 관찰하는 경우를 생각해 보십시오.$\mathbf{y}$ 레이블에 이전과 동일한 표현을 사용할 수 있습니다.유일한 차이점은 $(0, 0, 1)$과 같은 이진 항목만 포함하는 벡터가 아니라 이제 일반 확률 벡터 (예: $(0.1, 0.2, 0.7)$) 가 있다는 것입니다.이전에 :eqref:`eq_l_cross_entropy`에서 손실 $l$을 정의하기 위해 사용했던 수학은 해석이 약간 더 일반적이라는 점에서 여전히 잘 작동합니다.레이블에 대한 분포에 대한 손실의 예상 값입니다.이 손실을*교차 엔트로피 손실*이라고하며 분류 문제에서 가장 일반적으로 사용되는 손실 중 하나입니다.우리는 정보 이론의 기초만을 소개함으로써 이름을 이해할 수 있습니다.정보 이론에 대한 자세한 내용을 이해하려면 [online appendix on information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)를 참조하십시오. 

## 정보 이론 기초
:label:`subsec_info_theory_basics`

*정보 이론*은 인코딩, 디코딩, 전송,
가능한 한 간결한 형태로 정보 (데이터라고도 함) 를 조작하는 것입니다. 

### 엔트로피

정보 이론의 핵심 아이디어는 데이터의 정보 내용을 정량화하는 것입니다.이 수량은 데이터 압축 능력에 큰 제한을 둡니다.정보 이론에서 이 양을 분포 $P$의*엔트로피*라고 하며 다음 방정식으로 캡처됩니다. 

$$H[P] = \sum_j - P(j) \log P(j).$$
:eqlabel:`eq_softmax_reg_entropy`

정보 이론의 기본 정리 중 하나는 분포 $P$에서 무작위로 추출한 데이터를 인코딩하기 위해이를 인코딩하기 위해 최소 $H[P]$ “nats”가 필요하다고 말합니다.“nat”가 무엇인지 궁금하다면 비트와 동일하지만 기본 2가 아닌 기본 $e$의 코드를 사용할 때입니다.따라서 하나의 nat는 $\frac{1}{\log(2)} \approx 1.44$비트입니다. 

### 서프라이즈

압축이 예측과 어떤 관련이 있는지 궁금 할 것입니다.압축하려는 데이터 스트림이 있다고 상상해보십시오.다음 토큰을 항상 쉽게 예측할 수 있다면 이 데이터는 압축하기 쉽습니다!스트림의 모든 토큰이 항상 동일한 값을 취하는 극단적인 예를 들어 보겠습니다.이는 매우 지루한 데이터 스트림입니다!지루할 뿐만 아니라 예측하기도 쉽습니다.항상 동일하므로 스트림의 내용을 전달하기 위해 정보를 전송할 필요가 없습니다.예측하기 쉽고 압축하기 쉽습니다. 

그러나 모든 사건을 완벽하게 예측할 수 없다면 때때로 놀랄 수도 있습니다.이벤트에 더 낮은 확률을 할당했을 때 놀라움이 더 커집니다.클로드 섀넌은 $\log \frac{1}{P(j)} = -\log P(j)$에 정착하여 (주관적인) 확률 $P(j)$을 할당 한 사건 $j$을 관찰 할 때 자신의 놀라움*을 정량화했습니다.:eqref:`eq_softmax_reg_entropy`에 정의된 엔트로피는 데이터 생성 프로세스와 실제로 일치하는 정확한 확률을 할당했을 때*예상되는 놀라움*입니다. 

### 교차 엔트로피 재검토

따라서 엔트로피가 실제 확률을 아는 사람이 경험하는 놀라움의 수준이라면 교차 엔트로피가 무엇인지 궁금 할 것입니다.$H(P, Q)$로 표시되는* $P$*에서* $Q$까지의 교차 엔트로피는 확률 $P$에 따라 실제로 생성 된 데이터를 볼 때 주관적 확률 $Q$을 가진 관찰자의 예상되는 놀라움입니다.가능한 가장 낮은 교차 엔트로피는 $P=Q$이 될 때 달성됩니다.이 경우 $P$에서 $Q$까지의 교차 엔트로피는 $H(P, P)= H(P)$입니다. 

요컨대, 교차 엔트로피 분류 목표는 두 가지 방식으로 생각할 수 있습니다. (i) 관찰 된 데이터의 가능성을 극대화하는 것; (ii) 레이블을 전달하는 데 필요한 놀라움 (따라서 비트 수) 을 최소화하는 것입니다. 

## 모델 예측 및 평가

소프트맥스 회귀 모델을 훈련시킨 후 예제 기능이 주어지면 각 출력 클래스의 확률을 예측할 수 있습니다.일반적으로 예측 확률이 가장 높은 클래스를 출력 클래스로 사용합니다.실제 클래스 (레이블) 와 일치하면 예측이 정확합니다.실험의 다음 부분에서는*정확도*를 사용하여 모델의 성능을 평가합니다.이는 정확한 예측 수와 총 예측 수 간의 비율과 같습니다. 

## 요약

* 소프트맥스 연산은 벡터를 취하여 확률에 매핑합니다.
* 소프트맥스 회귀는 분류 문제에 적용됩니다.소프트맥스 연산에 출력 클래스의 확률 분포를 사용합니다.
* 교차 엔트로피는 두 확률 분포 간의 차이를 측정하는 좋은 척도입니다.주어진 모델에서 데이터를 인코딩하는 데 필요한 비트 수를 측정합니다.

## 연습문제

1. 지수 패밀리와 소프트맥스 간의 연관성을 좀 더 깊이 살펴볼 수 있습니다.
    1. 소프트맥스에 대한 교차 엔트로피 손실 $l(\mathbf{y},\hat{\mathbf{y}})$의 2차 도함수를 계산합니다.
    1. $\mathrm{softmax}(\mathbf{o})$로 지정된 분포의 분산을 계산하고 위에서 계산한 2차 도함수와 일치하는지 보여줍니다.
1. 동일한 확률로 발생하는 세 개의 클래스가 있다고 가정합니다. 즉, 확률 벡터는 $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$입니다.
    1. 바이너리 코드를 디자인하려고 하면 어떤 문제가 될까요?
    1. 더 나은 코드를 디자인할 수 있을까요?힌트: 두 개의 독립적인 관측값을 인코딩하려고 하면 어떻게 될까요?$n$개의 관측값을 공동으로 인코딩하면 어떻게 될까요?
1. Softmax는 위에서 소개한 매핑에 대한 잘못된 이름입니다 (하지만 딥 러닝의 모든 사용자가 사용합니다).실제 소프트맥스는 $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$로 정의됩니다.
    1. $\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$라는 것을 증명하십시오.
    1. $\lambda > 0$가 제공된다면 이것이 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$에 대해 유지된다는 것을 증명하십시오.
    1. $\lambda \to \infty$에 대해 우리는 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$를 가지고 있음을 보여줍니다.
    1. 소프트 민은 어떻게 생겼을까요?
    1. 이 값을 세 개 이상의 숫자로 확장합니다.

[Discussions](https://discuss.d2l.ai/t/46)
