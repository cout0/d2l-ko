# 완전 연결 계층에서 컨벌루션까지
:label:`sec_why-conv`

오늘날까지 지금까지 논의한 모델은 표 형식의 데이터를 다룰 때 적절한 옵션으로 남아 있습니다.표 형식이란 데이터가 예제에 해당하는 행과 특징에 해당하는 열로 구성됨을 의미합니다.테이블 형식 데이터를 사용하면 우리가 찾는 패턴이 특징 간의 상호 작용을 포함 할 수 있다고 예상 할 수 있지만 기능이 상호 작용하는 방식에 관한*선험적*구조는*선험적*이라고 가정하지 않습니다. 

때로는 더 정교한 건축물의 건설을 안내하는 데 필요한 지식이 부족할 때가 있습니다.이러한 경우 MLP가 최선일 수 있습니다.그러나 고차원 지각 데이터의 경우 이러한 구조가 없는 네트워크는 다루기 힘들 수 있습니다. 

예를 들어 고양이와 개를 구별하는 실행 사례로 돌아가 보겠습니다.주석이 달린 1메가픽셀 사진 데이터세트를 수집하면서 데이터 수집에 대한 철저한 작업을 수행한다고 가정해 보겠습니다.즉, 네트워크에 대한 각 입력에는 백만 개의 차원이 있습니다.:numref:`subsec_parameterization-cost-fc-layers`에서 완전히 연결된 레이어의 파라미터화 비용에 대한 논의에 따르면 1,000개의 숨겨진 차원으로 공격적으로 축소하더라도 $10^6 \times 10^3 = 10^9$ 파라미터로 특성화된 완전 연결 계층이 필요합니다.GPU가 많고 분산 최적화에 대한 재능과 엄청난 인내심이 없다면 이 네트워크의 매개 변수를 배우는 것이 불가능할 수 있습니다. 

주의 깊은 독자는 1메가픽셀 해상도가 필요하지 않을 수 있다는 근거로 이 주장에 반대할 수 있습니다.그러나 십만 픽셀로 벗어날 수는 있지만 크기가 1000 인 숨겨진 레이어는 이미지의 좋은 표현을 배우는 데 필요한 숨겨진 단위의 수를 크게 과소 평가하므로 실제 시스템에는 여전히 수십억 개의 매개 변수가 필요합니다.또한 너무 많은 매개 변수를 피팅하여 분류기를 학습하려면 방대한 데이터 세트를 수집해야 할 수 있습니다.그러나 오늘날 인간과 컴퓨터 모두 고양이와 개를 아주 잘 구별 할 수 있습니다. 이러한 직감과 모순되는 것처럼 보입니다.이미지는 인간과 기계 학습 모델 모두가 활용할 수 있는 풍부한 구조를 보여주기 때문입니다.컨벌루션 신경망 (CNN) 은 기계 학습이 자연 이미지에서 알려진 구조 중 일부를 활용하기 위해 수용한 창의적인 방법 중 하나입니다. 

## 불변성

이미지에서 물체를 감지하려고 한다고 가정해 보십시오.물체를 인식하기 위해 어떤 방법을 사용하든 이미지에서 물체의 정확한 위치에 지나치게 관여해서는 안된다는 것이 합리적입니다.이상적으로는 시스템이 이러한 지식을 활용해야 합니다.돼지는 보통 날지 않으며 비행기는 보통 수영하지 않습니다.그럼에도 불구하고 우리는 여전히 돼지가 이미지 상단에 나타났다는 것을 인식해야 합니다.어린이 게임 “Where's Waldo” (:numref:`img_waldo`에 묘사 됨) 에서 영감을 얻을 수 있습니다.이 게임은 활동으로 가득 찬 수많은 혼란스러운 장면으로 구성됩니다.Waldo는 각각의 어딘가에 나타나며 일반적으로 예상치 못한 위치에 숨어 있습니다.독자의 목표는 그를 찾는 것입니다.그의 특징적인 복장에도 불구하고 많은 산만 함으로 인해 놀라 울 정도로 어려울 수 있습니다.그러나*Waldo의 모습*은*Waldo의 위치*에 의존하지 않습니다.각 패치에 점수를 할당할 수 있는 Waldo 감지기로 이미지를 스윕하여 패치에 Waldo가 포함될 가능성을 나타낼 수 있습니다.CNN은*공간 불변성*이라는 개념을 체계화하여 더 적은 매개 변수로 유용한 표현을 학습합니다. 

![An image of the "Where's Waldo" game.](../img/where-wally-walker-books.jpg)
:width:`400px`
:label:`img_waldo`

이제 컴퓨터 비전에 적합한 신경망 아키텍처 설계를 안내하기 위해 몇 가지 desiderata를 열거하여 이러한 직관을 보다 구체적으로 만들 수 있습니다. 

1. 초기 계층에서는 네트워크가 이미지의 어디에 나타나는지에 관계없이 동일한 패치에 유사하게 반응해야 합니다.이 원칙을*번역 불변성*이라고 합니다.
1. 네트워크의 가장 초기 계층은 먼 지역의 이미지 내용에 관계없이 로컬 영역에 초점을 맞춰야 합니다.이것이*지역* 원칙입니다.결국 이러한 로컬 표현을 집계하여 전체 이미지 수준에서 예측할 수 있습니다.

이것이 어떻게 수학으로 변환되는지 봅시다. 

## MLP 제약하기

우선, 2 차원 이미지 $\mathbf{X}$를 입력으로 사용하고 즉각적인 숨겨진 표현 $\mathbf{H}$를 수학에서 행렬과 코드의 2 차원 텐서로 유사하게 표현되는 MLP를 고려할 수 있습니다. $\mathbf{X}$와 $\mathbf{H}$는 모두 동일한 모양을 갖습니다.저게 가라 앉게 해줘이제 우리는 입력뿐만 아니라 숨겨진 표현도 공간 구조를 가지고 있다고 생각합니다. 

$[\mathbf{X}]_{i, j}$ 및 $[\mathbf{H}]_{i, j}$는 각각 입력 이미지와 숨겨진 표현에서 위치 ($i$, $j$) 의 픽셀을 나타냅니다.결과적으로 각 숨겨진 단위가 각 입력 픽셀에서 입력을 받도록하기 위해 가중치 행렬 (이전에 MLP에서했던 것처럼) 을 사용하는 것에서 매개 변수를 4 차 가중치 텐서 $\mathsf{W}$으로 표현하는 것으로 전환합니다.$\mathbf{U}$에 편향이 있다고 가정하면 완전히 연결된 계층을 다음과 같이 공식적으로 표현할 수 있습니다. 

$$\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned},$$

여기서 $\mathsf{W}$에서 $\mathsf{V}$로의 전환은 두 4 차 텐서의 계수 사이에 일대일 대응이 있기 때문에 현재로서는 완전히 외관적입니다.우리는 단순히 $k = i+a$ 및 $l = j+b$가 되도록 아래 첨자 $(k, l)$를 다시 색인화합니다.즉, 우리는 $[\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}$를 설정했습니다.인덱스 $a$ 및 $b$은 양수 오프셋과 음수 오프셋 모두에서 실행되며 전체 이미지를 포함합니다.숨겨진 표현 $[\mathbf{H}]_{i, j}$의 주어진 위치 ($i$, $j$) 에 대해 $x$의 픽셀을 합산하고 $(i, j)$를 중심으로 가중치를 부여하여 값을 계산합니다. 

### 번역 불변성

이제 위에서 설정한 첫 번째 원칙인 번역 불변성을 호출해 보겠습니다.이는 입력 $\mathbf{X}$의 이동이 단순히 숨겨진 표현 $\mathbf{H}$의 이동으로 이어져야 함을 의미합니다.이것은 $\mathsf{V}$와 $\mathbf{U}$가 실제로 $(i, j)$에 의존하지 않는 경우에만 가능합니다. 즉, 우리는 $[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$을 가지고 있고 $\mathbf{U}$는 상수입니다 (예: $u$).결과적으로 $\mathbf{H}$에 대한 정의를 단순화할 수 있습니다. 

$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$

이것은*컨벌루션*입니다!우리는 값 $[\mathbf{H}]_{i, j}$을 얻기 위해 계수 $[\mathbf{V}]_{a, b}$을 사용하여 위치 $(i, j)$ 부근의 $(i+a, j+b)$에서 픽셀에 효과적으로 가중치를 부여하고 있습니다.$[\mathbf{V}]_{a, b}$은 더 이상 이미지 내의 위치에 의존하지 않으므로 $[\mathsf{V}]_{i, j, a, b}$보다 훨씬 적은 계수가 필요합니다.우리는 상당한 진전을 이루었습니다! 

###  소재지

이제 두 번째 원칙 인 지역성을 생각해 보겠습니다.위에서 동기를 부여한 바와 같이 $[\mathbf{H}]_{i, j}$에서 무슨 일이 일어나고 있는지 평가하기 위해 관련 정보를 수집하기 위해 $(i, j)$ 위치에서 멀리 볼 필요는 없다고 생각합니다.즉, 일부 범위 $|a|> \Delta$ 또는 $|b| > \Delta$을 벗어나면 $[\mathbf{V}]_{a, b} = 0$를 설정해야 합니다.마찬가지로 $[\mathbf{H}]_{i, j}$을 다음과 같이 다시 작성할 수 있습니다. 

$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$
:eqlabel:`eq_conv-layer`

간단히 말해서 :eqref:`eq_conv-layer`는*컨벌루션 계층*입니다.
*컨벌루션 신경망* (CNN)
는 컨벌루션 계층을 포함하는 특수한 신경망 제품군입니다.딥러닝 연구 커뮤니티에서 $\mathbf{V}$는*컨볼루션 커널*, *필터* 또는 단순히 학습 가능한 파라미터인 레이어의*가중치*라고 합니다.로컬 지역이 작은 경우 완전히 연결된 네트워크와 비교할 때 차이가 클 수 있습니다.이전에는 이미지 처리 네트워크에서 단일 계층을 나타내는 데 수십억 개의 매개 변수가 필요했지만 이제는 입력이나 숨겨진 표현의 차원을 변경하지 않고 일반적으로 수백 개만 필요합니다.이러한 파라미터의 급격한 감소에 대한 대가는 이제 기능이 번역이 불변하며 숨겨진 각 활성화의 가치를 결정할 때 계층이 로컬 정보만 통합할 수 있다는 것입니다.모든 학습은 귀납적 편향을 부과하는 것에 달려 있습니다.이러한 편향이 현실과 일치하면 보이지 않는 데이터로 잘 일반화되는 샘플 효율적인 모델을 얻을 수 있습니다.그러나 물론 이러한 편견이 현실과 일치하지 않는 경우 (예: 이미지가 번역 불변성이 아닌 것으로 판명 된 경우) 모델은 학습 데이터를 맞추는 데 어려움을 겪을 수 있습니다. 

## 컨볼루션

더 진행하기 전에 위의 연산이 컨볼루션이라고 불리는 이유를 간단히 검토해야 합니다.수학에서 두 함수 사이의*컨벌루션* (예: $f, g: \mathbb{R}^d \to \mathbb{R}$) 은 다음과 같이 정의됩니다. 

$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$

즉, 한 함수가 “반전”되고 $\mathbf{x}$만큼 이동될 때 $f$와 $g$ 사이의 겹침을 측정합니다.이산 객체가 있을 때마다 적분은 합으로 바뀝니다.예를 들어, 인덱스가 $\mathbb{Z}$을 초과하는 제곱합 가능한 무한 차원 벡터 집합의 벡터에 대해 다음 정의를 얻습니다. 

$$(f * g)(i) = \sum_a f(a) g(i-a).$$

2차원 텐서의 경우 각각 $f$에 대해 인덱스 $(a, b)$과 $g$에 대해 $(i-a, j-b)$의 해당 합계가 있습니다. 

$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:eqlabel:`eq_2d-conv-discrete`

이는 :eqref:`eq_conv-layer`와 비슷하지만 한 가지 큰 차이점이 있습니다.$(i+a, j+b)$을 사용하는 대신 차이점을 사용하고 있습니다.하지만 :eqref:`eq_conv-layer`와 :eqref:`eq_2d-conv-discrete` 사이의 표기법을 항상 일치시킬 수 있기 때문에 이러한 구분은 대부분 외관적입니다.:eqref:`eq_conv-layer`의 원래 정의는*상호 상관 관계*를 더 적절하게 설명합니다.다음 섹션에서 다시 설명하겠습니다. 

## “어디 왈도” 재방문

Waldo 탐지기로 돌아가서 어떻게 생겼는지 봅시다.컨벌루션 계층은 주어진 크기의 윈도우를 선택하고 :numref:`fig_waldo_mask`에 설명된 대로 필터 $\mathsf{V}$에 따라 강도의 가중치를 부여합니다.“waldoness”가 가장 높은 곳이면 어디든 숨겨진 레이어 표현에서 피크를 찾을 수 있도록 모델을 배우는 것을 목표로 할 수 있습니다. 

![Detect Waldo.](../img/waldo-mask.jpg)
:width:`400px`
:label:`fig_waldo_mask`

### 채널
:label:`subsec_why-conv-channels`

이 접근법에는 한 가지 문제점이 있습니다.지금까지 이미지가 빨강, 녹색, 파랑의 세 가지 채널로 구성되어 있다는 사실을 무시했습니다.실제로 이미지는 2차원 물체가 아니라 높이, 너비 및 채널 (예: 모양 $1024 \times 1024 \times 3$ 픽셀) 을 특징으로하는 3 차 텐서입니다.이러한 축 중 처음 두 축은 공간 관계와 관련이 있지만 세 번째 축은 각 픽셀 위치에 다차원 표현을 할당하는 것으로 간주 될 수 있습니다.따라서 우리는 $\mathsf{X}$를 $[\mathsf{X}]_{i, j, k}$으로 색인화합니다.컨벌루션 필터는 그에 따라 적응해야 합니다.$[\mathbf{V}]_{a,b}$ 대신, 우리는 이제 $[\mathsf{V}]_{a,b,c}$를 가지고 있습니다. 

또한 입력이 3 차 텐서로 구성된 것처럼 숨겨진 표현을 3 차 텐서 $\mathsf{H}$로 유사하게 공식화하는 것이 좋습니다.즉, 각 공간 위치에 해당하는 단일 숨겨진 표현을 갖는 것이 아니라 각 공간 위치에 해당하는 숨겨진 표현의 전체 벡터를 원합니다.숨겨진 표현은 서로 겹쳐진 여러 개의 2차원 격자로 구성된 것으로 생각할 수 있습니다.입력에서와 같이 이들을 때때로*채널*이라고 합니다.각 맵은 학습된 피처의 공간화된 세트를 후속 레이어에 제공하기 때문에*피처 맵*이라고도 합니다.직관적으로 입력에 더 가까운 하위 레이어에서는 일부 채널이 가장자리를 인식하도록 특수화되고 다른 채널은 텍스처를 인식할 수 있다고 상상할 수 있습니다. 

입력 ($\mathsf{X}$) 과 숨겨진 표현 ($\mathsf{H}$) 모두에서 여러 채널을 지원하기 위해 $\mathsf{V}$:$[\mathsf{V}]_{a, b, c, d}$에 네 번째 좌표를 추가할 수 있습니다.모든 것을 하나로 모으는 것: 

$$[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},$$
:eqlabel:`eq_conv-layer-channels`

여기서 $d$은 숨겨진 표현 $\mathsf{H}$에서 출력 채널을 인덱싱합니다.후속 컨벌루션 계층은 계속해서 3차 텐서인 $\mathsf{H}$를 입력으로 사용합니다.보다 일반적으로 :eqref:`eq_conv-layer-channels`는 다중 채널에 대한 컨벌루션 계층의 정의입니다. 여기서 $\mathsf{V}$은 계층의 커널 또는 필터입니다. 

아직 해결해야 할 작업이 많이 있습니다.예를 들어, 모든 숨겨진 표현을 단일 출력으로 결합하는 방법을 알아내야 합니다 (예: 이미지에 Waldo*anywhere*가 있는지 여부).또한 사물을 효율적으로 계산하는 방법, 여러 계층을 결합하는 방법, 적절한 활성화 함수 및 실제로 효과적인 네트워크를 생성하기 위해 합리적인 설계 선택을 하는 방법을 결정해야 합니다.이 장의 나머지 부분에서 이러한 문제를 다룹니다. 

## 요약

* 이미지의 변환 불변성은 이미지의 모든 패치가 동일한 방식으로 처리됨을 의미합니다.
* 지역성은 픽셀의 작은 이웃만 해당 숨겨진 표현을 계산하는 데 사용됨을 의미합니다.
* 이미지 처리에서 컨벌루션 계층은 일반적으로 완전 연결 계층보다 훨씬 적은 수의 파라미터를 필요로 합니다.
* CNN은 컨벌루션 계층을 포함하는 특수한 신경망 제품군입니다.
* 입력 및 출력 채널을 통해 모델은 각 공간 위치에서 이미지의 여러 측면을 캡처할 수 있습니다.

## 연습문제

1. 컨볼루션 커널의 크기가 $\Delta = 0$라고 가정합니다.이 경우 컨볼루션 커널이 각 채널 집합에 대해 MLP를 독립적으로 구현한다는 것을 보여줍니다.
1. 결국 번역 불변성이 좋은 생각이 아닌 이유는 무엇입니까?
1. 이미지 경계에서 픽셀 위치에 해당하는 숨겨진 표현을 처리하는 방법을 결정할 때 어떤 문제를 해결해야 합니까?
1. 오디오에 대한 유사한 컨벌루션 계층을 설명합니다.
1. 컨벌루션 계층이 텍스트 데이터에도 적용될 수 있다고 생각하십니까?왜, 왜 안되니?
1. $f * g = g * f$라는 것을 증명하십시오.

[Discussions](https://discuss.d2l.ai/t/64)
