# 주의 메커니즘
:label:`chap_attention`

영장류의 시각 시스템의 시신경은 뇌가 완전히 처리 할 수있는 것보다 훨씬 많은 감각 입력을 받습니다.다행히도 모든 자극이 똑같이 만들어지는 것은 아닙니다.의식의 집중과 집중으로 인해 영장류는 복잡한 시각적 환경에서 먹이와 포식자와 같은 관심 대상에주의를 기울일 수있었습니다.정보의 극히 일부에만 주의를 기울이는 능력은 진화적인 의미를 지니며, 인간이 살아가고 성공할 수 있도록 합니다. 

과학자들은 19세기부터 인지 신경 과학 분야에서 관심을 연구해 왔습니다.이 장에서는 시각적 장면에서 주의가 어떻게 전개되는지 설명하는 인기 있는 프레임워크를 검토하는 것으로 시작하겠습니다.이 프레임워크의 주의 신호에서 영감을 받아 이러한 주의 신호를 활용하는 모델을 설계할 것입니다.특히 1964년 나다라야-와스톤 커널 회귀는*주의 메커니즘*을 사용한 기계 학습의 간단한 데모입니다. 

다음으로, 딥러닝에서 주의력 모델 설계에 광범위하게 사용된 주의력 함수를 소개하겠습니다.구체적으로, 이러한 함수를 사용하여 양방향으로 정렬하고 차별화할 수 있는 딥 러닝의 획기적인 주의 모델인*Bahdanau attention*을 설계하는 방법을 보여드리겠습니다. 

결국 최신 장비를 갖추고 있습니다.
*멀티 헤드 주의*
그리고*자기 주의력* 설계에 대해서는 주의 메커니즘만을 기반으로*변압기* 아키텍처를 설명합니다.2017년 제안 이후 트랜스포머는 언어, 시각, 언어 및 강화 학습 분야와 같은 최신 딥 러닝 애플리케이션에 널리 보급되어 왔습니다.

```toc
:maxdepth: 2

attention-cues
nadaraya-watson
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
