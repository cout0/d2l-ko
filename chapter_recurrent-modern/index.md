# 최신 순환 신경망
:label:`chap_modern_rnn`

시퀀스 데이터를 더 잘 처리할 수 있는 RNN의 기본 사항을 소개했습니다.데모를 위해 텍스트 데이터에 RNN 기반 언어 모델을 구현했습니다.그러나 이러한 기술은 오늘날 광범위한 시퀀스 학습 문제에 직면 할 때 실무자에게 충분하지 않을 수 있습니다. 

예를 들어, 실제로 주목할만한 문제는 RNN의 수치적 불안정성입니다.그래디언트 클리핑과 같은 구현 트릭을 적용했지만 시퀀스 모델의 보다 정교한 설계를 통해 이 문제를 더욱 완화할 수 있습니다.특히 게이트 RNN은 실제로 훨씬 더 일반적입니다.먼저 널리 사용되는 두 개의 네트워크, 즉*게이트 순환 단위* (GRU) 와*장기 단기 메모리* (LSTM) 를 소개합니다.또한 지금까지 논의한 단일 단방향 히든 레이어로 RNN 아키텍처를 확장할 예정입니다.여러 개의 숨겨진 계층이있는 심층 아키텍처를 설명하고 순방향 및 역방향 반복 계산을 모두 사용하여 양방향 설계에 대해 논의합니다.이러한 확장은 현대의 순환 네트워크에서 자주 채택됩니다.이러한 RNN 변형을 설명할 때 :numref:`chap_rnn`에 도입된 것과 동일한 언어 모델링 문제를 계속 고려합니다. 

실제로 언어 모델링은 시퀀스 학습이 할 수있는 것의 극히 일부만 보여줍니다.자동 음성 인식, 텍스트 음성 변환 및 기계 번역과 같은 다양한 시퀀스 학습 문제에서 입력과 출력은 모두 임의의 길이의 시퀀스입니다.이러한 유형의 데이터를 맞추는 방법을 설명하기 위해 기계 번역을 예로 들어 RNN 및 시퀀스 생성을 위한 빔 검색을 기반으로 한 인코더-디코더 아키텍처를 소개합니다.

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
