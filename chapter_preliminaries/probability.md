# 확률
:label:`sec_prob`

어떤 형태로든 기계 학습은 예측에 관한 것입니다.임상 병력을 고려할 때 내년에 심장 마비로 고통받는 환자의*확률*을 예측하고 싶을 수 있습니다.이상 탐지에서는 비행기의 제트 엔진에서 나온 일련의 판독값이 정상적으로 작동하고 있는지*가능성*을 평가할 수 있습니다.강화 학습에서는 에이전트가 환경에서 지능적으로 행동하기를 원합니다.즉, 사용 가능한 각 행동에서 높은 보상을 받을 확률을 고려해야 합니다.추천자 시스템을 구축할 때는 확률에 대해서도 생각해야 합니다.예를 들어, 대규모 온라인 서점에서 일했다고 가정적으로*가정적으로* 말합니다.특정 사용자가 특정 책을 구매할 확률을 추정할 수 있습니다.이를 위해서는 확률 언어를 사용해야합니다.전체 과정, 전공, 논문, 경력, 심지어 부서까지도 확률에 전념합니다.당연히 이 섹션의 목표는 전체 주제를 가르치는 것이 아닙니다.대신, 첫 번째 딥 러닝 모델 구축을 시작할 수 있을 만큼 충분히 가르치고, 원하는 경우 직접 탐색할 수 있는 주제에 대한 충분한 풍미를 제공할 수 있기를 바랍니다. 

이전 섹션에서 확률을 정확하게 설명하거나 구체적인 예를 제시하지 않고 이미 확률을 호출했습니다.사진을 기반으로 고양이와 개를 구별하는 첫 번째 사례를 고려하여 이제 더 진지하게 생각해 봅시다.간단하게 들릴지 모르지만 실제로는 엄청난 도전입니다.우선 문제의 난이도는 이미지의 해상도에 따라 달라질 수 있습니다. 

![Images of varying resolutions ($10 \times 10$, $20 \times 20$, $40 \times 40$, $80 \times 80$, and $160 \times 160$ pixels).](../img/cat-dog-pixels.png)
:width:`300px`
:label:`fig_cat_dog`

:numref:`fig_cat_dog`에서 볼 수 있듯이 인간은 $160 \times 160$ 픽셀의 해상도로 고양이와 개를 쉽게 인식 할 수 있지만 $40 \times 40$ 픽셀에서는 어려워지고 $10 \times 10$ 픽셀에서는 불가능합니다.즉, 고양이와 개를 먼 거리 (따라서 낮은 해상도) 로 구별하는 능력은 정보에 입각하지 않은 추측에 접근 할 수 있습니다.확률은 확실성 수준에 대한 공식적인 추론 방법을 제공합니다.이미지가 고양이를 묘사한다고 완전히 확신한다면 해당 레이블 $y$이 “고양이”인 “고양이”일 확률*이 $)$가 $1$과 같다고 말합니다.$y =$ “고양이”또는 $y =$ “개”를 암시하는 증거가 없다면 두 가지 가능성이 동일하다고 말할 수 있습니다.
*아마도 이것을 $P(y=$ “고양이” $) = P(y=$ “개” $) = 0.5$으로 표현할 가능성이 높습니다.우리가 합리적으로
자신감이 있지만 이미지가 고양이를 묘사했는지 확실하지 않은 경우 확률 $0.5  < P(y=$ “고양이”$) < 1$를 할당 할 수 있습니다. 

이제 두 번째 경우를 생각해보십시오. 일부 기상 모니터링 데이터가 주어지면 내일 타이페이에서 비가 올 확률을 예측하고 싶습니다.여름철이면 비가 0.5 확률로 올 수 있습니다. 

두 경우 모두 관심의 가치가 있습니다.두 경우 모두 결과에 대해 불확실합니다.그러나 두 사례 간에는 중요한 차이점이 있습니다.이 첫 번째 경우 이미지는 실제로 개나 고양이이며 어떤 이미지인지 알 수 없습니다.두 번째 경우, 그러한 것들을 믿는다면 (그리고 대부분의 물리학자들이 그렇게합니다) 결과는 실제로 무작위 사건이 될 수 있습니다.따라서 확률은 우리의 확실성 수준에 대해 추론할 수 있는 유연한 언어이며, 광범위한 맥락에서 효과적으로 적용될 수 있습니다. 

## 기본 확률 이론

주사위를 던지고 다른 숫자가 아닌 1을 볼 가능성이 무엇인지 알고 싶다고 가정 해보십시오.다이가 공정하다면 여섯 가지 결과 $\{1, \ldots, 6\}$이 모두 똑같이 발생할 가능성이 높으므로 여섯 가지 사례 중 하나에서 $1$를 볼 수 있습니다.공식적으로 우리는 $1$가 확률 $\frac{1}{6}$로 발생한다고 명시합니다. 

공장에서 받은 실제 다이의 경우 그 비율을 알지 못할 수 있으며 오염되었는지 확인해야 합니다.다이를 조사하는 유일한 방법은 여러 번 주조하고 결과를 기록하는 것입니다.다이의 각 캐스트에 대해 $\{1, \ldots, 6\}$의 값을 관찰합니다.이러한 결과를 감안할 때 각 결과를 관찰 할 확률을 조사하고자합니다. 

각 값에 대한 자연스러운 접근 방식 중 하나는 해당 값에 대한 개별 카운트를 취하여 총 던지기 횟수로 나누는 것입니다.이를 통해 주어진*사건*확률의*추정치*를 얻을 수 있습니다.*큰 수의 법칙*은 던지기 횟수가 증가함에 따라 이 추정치가 실제 기본 확률에 점점 더 가까워진다는 것을 알려줍니다.여기서 무슨 일이 벌어지고 있는지 자세히 살펴보기 전에 시도해 보겠습니다. 

시작하려면 필요한 패키지를 가져옵니다.

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
from mxnet import np, npx
import random
npx.set_np()
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch.distributions import multinomial
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np
```

다음으로, 우리는 다이를 던질 수 있기를 원할 것입니다.통계에서는 확률 분포*샘플링*에서 예제를 그리는 이 과정을 호출합니다.여러 이산형 선택 항목에 확률을 할당하는 분포를
*다항 분포*.보다 공식적인 정의를 내릴 것입니다.
*배포* 나중에, 하지만 높은 수준에서는 배포판을
사건에 대한 확률. 

단일 표본을 그리려면 확률로 구성된 벡터를 전달하기만 하면 됩니다.출력값은 길이가 같은 다른 벡터입니다. 인덱스 $i$의 값은 샘플링 결과가 $i$에 해당하는 횟수입니다.

```{.python .input}
fair_probs = [1.0 / 6] * 6
np.random.multinomial(1, fair_probs)
```

```{.python .input}
#@tab pytorch
fair_probs = torch.ones([6]) / 6
multinomial.Multinomial(1, fair_probs).sample()
```

```{.python .input}
#@tab tensorflow
fair_probs = tf.ones(6) / 6
tfp.distributions.Multinomial(1, fair_probs).sample()
```

샘플러를 여러 번 실행하면 매번 임의의 값을 얻을 수 있습니다.다이의 공정성을 추정할 때와 마찬가지로 동일한 분포에서 많은 표본을 생성하고자 하는 경우가 많습니다.Python `for` 루프로 이 작업을 수행하는 것은 참을 수 없을 정도로 느릴 수 있으므로 우리가 사용하는 함수는 한 번에 여러 샘플을 그리는 것을 지원하여 원하는 모양의 독립 샘플 배열을 반환합니다.

```{.python .input}
np.random.multinomial(10, fair_probs)
```

```{.python .input}
#@tab pytorch
multinomial.Multinomial(10, fair_probs).sample()
```

```{.python .input}
#@tab tensorflow
tfp.distributions.Multinomial(10, fair_probs).sample()
```

이제 다이 롤을 샘플링하는 방법을 알았으므로 1000개의 롤을 시뮬레이션할 수 있습니다.그런 다음 1000 개의 롤 후에 각 숫자가 몇 번 굴러졌는지 살펴보고 계산할 수 있습니다.특히 상대 빈도를 실제 확률의 추정치로 계산합니다.

```{.python .input}
counts = np.random.multinomial(1000, fair_probs).astype(np.float32)
counts / 1000
```

```{.python .input}
#@tab pytorch
# Store the results as 32-bit floats for division
counts = multinomial.Multinomial(1000, fair_probs).sample()
counts / 1000  # Relative frequency as the estimate
```

```{.python .input}
#@tab tensorflow
counts = tfp.distributions.Multinomial(1000, fair_probs).sample()
counts / 1000
```

공정 다이에서 데이터를 생성했기 때문에 각 결과에 실제 확률 $\frac{1}{6}$ (대략 $0.167$) 가 있으므로 위의 출력 추정치가 양호해 보입니다. 

또한 이러한 확률이 시간이 지남에 따라 실제 확률로 수렴하는 방식을 시각화할 수 있습니다.각 그룹이 10개의 샘플을 추출하는 500개의 실험 그룹을 수행해 보겠습니다.

```{.python .input}
counts = np.random.multinomial(10, fair_probs, size=500)
cum_counts = counts.astype(np.float32).cumsum(axis=0)
estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)

d2l.set_figsize((6, 4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].asnumpy(),
                 label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
```

```{.python .input}
#@tab pytorch
counts = multinomial.Multinomial(10, fair_probs).sample((500,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)

d2l.set_figsize((6, 4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
```

```{.python .input}
#@tab tensorflow
counts = tfp.distributions.Multinomial(10, fair_probs).sample(500)
cum_counts = tf.cumsum(counts, axis=0)
estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)

d2l.set_figsize((6, 4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
```

각 솔리드 곡선은 다이의 6개 값 중 하나에 해당하며, 각 실험 그룹 후에 평가된 대로 다이가 해당 값을 올릴 것으로 추정된 확률을 제공합니다.검은색 파선은 실제 기본 확률을 나타냅니다.더 많은 실험을 수행하여 더 많은 데이터를 얻을수록 $6$ 솔리드 곡선이 실제 확률로 수렴합니다. 

### 확률 이론의 공리

다이의 롤을 다룰 때 세트 $\mathcal{S} = \{1, 2, 3, 4, 5, 6\}$를*샘플 공간* 또는*결과 공간*이라고 부릅니다. 여기서 각 요소는*결과*입니다.*event*는 주어진 표본 공간의 결과 집합입니다.예를 들어, “$5$ 보기” ($\{5\}$) 와 “홀수 보기” ($\{1, 3, 5\}$) 는 모두 다이를 굴리는 데 유효한 이벤트입니다.무작위 실험의 결과가 사건 $\mathcal{A}$에 있는 경우 사건 $\mathcal{A}$이 발생한 것입니다.즉, $3 \in \{1, 3, 5\}$ 이후 주사위를 굴린 후 $3$ 점이 마주 보게되면 “홀수를 보는”이벤트가 발생했다고 말할 수 있습니다. 

공식적으로*확률*은 집합을 실제 값에 매핑하는 함수로 생각할 수 있습니다.$P(\mathcal{A})$로 표시된 주어진 표본 공간 $\mathcal{S}$에서 사건 $\mathcal{A}$이 발생할 확률은 다음 특성을 충족합니다. 

* 모든 사건 $\mathcal{A}$에 대해 확률은 결코 음수가 아닙니다. 즉, $P(\mathcal{A}) \geq 0$입니다.
* 전체 샘플 공간의 확률은 $1$, 즉 $P(\mathcal{S}) = 1$입니다.
* *상호 배타적* (모든 $i \neq j$에 대해 $\mathcal{A}_i \cap \mathcal{A}_j = \emptyset$) 인 계산 가능한 사건 시퀀스 $\mathcal{A}_1, \mathcal{A}_2, \ldots$에 대해 발생할 확률은 개별 확률의 합, 즉 $P(\bigcup_{i=1}^{\infty} \mathcal{A}_i) = \sum_{i=1}^{\infty} P(\mathcal{A}_i)$과 같습니다.

이들은 또한 1933 년 콜모고로프가 제안한 확률 이론의 공리입니다.이 공리 체계 덕분에 무작위성에 대한 철학적 논쟁을 피할 수 있습니다. 대신 수학적 언어로 엄격하게 추론 할 수 있습니다.예를 들어, 사건 $\mathcal{A}_1$를 전체 표본 공간으로, 모든 $i > 1$에 대해 $\mathcal{A}_i = \emptyset$을 지정하면 $P(\emptyset) = 0$, 즉 불가능한 사건의 확률이 $0$이라는 것을 증명할 수 있습니다. 

### 랜덤 변수

다이를 주조하는 무작위 실험에서*랜덤 변수*라는 개념을 도입했습니다.랜덤 변수는 거의 모든 수량이 될 수 있으며 결정적이지 않습니다.무작위 실험에서 일련의 가능성 중 하나의 값을 취할 수 있습니다.다이를 굴리는 표본 공간 $\mathcal{S} = \{1, 2, 3, 4, 5, 6\}$에 값이 있는 랜덤 변수 $X$을 가정해 보겠습니다.“$5$를 보는 것”을 $\{X = 5\}$ 또는 $X = 5$로, 확률을 $P(\{X = 5\})$ 또는 $P(X = 5)$로 나타낼 수 있습니다.$P(X = a)$에 의해 랜덤 변수 $X$과 $X$이 취할 수 있는 값 (예: $a$) 을 구분합니다.그러나 이러한 보행자는 번거로운 표기법을 초래합니다.간결한 표기법의 경우 랜덤 변수 $X$에 대해*분포*로 $P(X)$를 나타낼 수 있습니다. 분포는 $X$이 값을 취할 확률을 알려줍니다.반면에 랜덤 변수가 $a$ 값을 취할 확률을 나타 내기 위해 $P(a)$을 간단히 작성할 수 있습니다.확률 이론의 사건은 표본 공간의 결과 집합이므로 랜덤 변수가 취할 값 범위를 지정할 수 있습니다.예를 들어, $P(1 \leq X \leq 3)$는 사건 $\{1 \leq X \leq 3\}$의 확률을 나타내며, 이는 $\{X = 1, 2, \text{or}, 3\}$를 의미합니다.마찬가지로 $P(1 \leq X \leq 3)$는 랜덤 변수 $X$이 $\{1, 2, 3\}$의 값을 취할 수 있는 확률을 나타냅니다. 

주사위의 측면과 같은*이산* 확률 변수와 사람의 몸무게와 키와 같은*연속* 변수 사이에는 미묘한 차이가 있습니다.두 사람의 키가 정확히 같은지 묻는 것은 의미가 거의 없습니다.충분히 정확하게 측정하면 지구상의 두 사람이 정확히 같은 키를 가지고 있지 않다는 것을 알게 될 것입니다.사실, 우리가 충분히 세밀하게 측정하면 잠에서 깨어날 때와 잠들 때 키가 같지 않을 것입니다.따라서 누군가의 키가 1.80139278291028719210196740527486202 미터일 확률에 대해 물어볼 목적이 없습니다.전 세계 인간 인구를 감안할 때 확률은 거의 0입니다.이 경우 누군가의 키가 주어진 간격 (예: 1.79 ~ 1.81 미터) 에 속하는지 묻는 것이 더 합리적입니다.이 경우 값을*밀도*로 볼 가능성을 정량화합니다.정확히 1.80m의 높이는 확률이 없지만 밀도는 0이 아닙니다.서로 다른 두 높이 사이의 간격에서 우리는 0이 아닌 확률을 갖습니다.이 섹션의 나머지 부분에서는 이산 공간의 확률을 고려합니다.계량형 랜덤 변수에 대한 확률은 :numref:`sec_random_variables`를 참조할 수 있습니다. 

## 다중 랜덤 변수 다루기

종종 우리는 한 번에 하나 이상의 랜덤 변수를 고려하기를 원할 것입니다.예를 들어 질병과 증상 간의 관계를 모델링할 수 있습니다.“독감”과 “기침”과 같은 질병과 증상이 주어지면 어느 정도의 확률로 환자에게 발생할 수도 있고 발생하지 않을 수도 있습니다.두 가지 확률이 0에 가까워지기를 바라지만, 더 나은 의료에 영향을 미치기 위해 추론을 적용 할 수 있도록 이러한 확률과 서로의 관계를 추정하고 싶을 수 있습니다. 

좀 더 복잡한 예로, 이미지에는 수백만 개의 픽셀이 포함되므로 수백만 개의 랜덤 변수가 포함됩니다.그리고 대부분의 경우 이미지에는 이미지의 개체를 식별하는 레이블이 함께 제공됩니다.레이블을 랜덤 변수로 생각할 수도 있습니다.모든 메타 데이터를 위치, 시간, 조리개, 초점 거리, ISO, 초점 거리 및 카메라 유형과 같은 임의의 변수로 생각할 수도 있습니다.이 모든 것은 공동으로 발생하는 랜덤 변수입니다.여러 랜덤 변수를 다룰 때 몇 가지 관심 수량이 있습니다. 

### 관절 확률

첫 번째는*관절 확률* $P(A = a, B=b)$라고 합니다.$a$와 $b$의 값이 주어지면 접합 확률을 통해 답을 얻을 수 있습니다. $A=a$과 $B=b$이 동시에 발생할 확률은 얼마입니까?모든 값에 대해 $a$ 및 $b$, $P(A=a, B=b) \leq P(A=a)$에 유의하십시오.$A=a$과 $B=b$이 발생하려면 $A=a$이 발생해야하고* $B=b$도 발생해야하기 때문에 (그리고 그 반대의 경우도 마찬가지입니다).따라서 $A=a$ 및 $B=b$은 개별적으로 $A=a$ 또는 $B=b$보다 높을 수 없습니다. 

### 조건부 확률

이것은 우리에게 흥미로운 비율을 가져다줍니다: $0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1$.이 비율을*조건부 확률*이라고 부르며 $P(B=b \mid A=a)$로 나타냅니다. $A=a$가 발생한 경우 $B=b$의 확률입니다. 

### 베이즈 정리

조건부 확률의 정의를 사용하여 통계에서 가장 유용하고 유명한 방정식 중 하나 인 *베이즈 정리*를 도출 할 수 있습니다.다음과 같이 진행됩니다.건설에 의해, 우리는 $P(A, B) = P(B \mid A) P(A)$*곱셈 규칙*을 가지고 있습니다.대칭으로 이것은 $P(A, B) = P(A \mid B) P(B)$에도 적용됩니다.$P(B) > 0$이라고 가정합니다.우리가 얻는 조건부 변수 중 하나에 대해 풀기 

$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.$$

여기서는 $P(A, B)$가*접합 분포*이고 $P(A \mid B)$은*조건부 분포*인 보다 간결한 표기법을 사용합니다.이러한 분포는 특정 값 $A = a, B=b$에 대해 평가할 수 있습니다. 

### 소외

베이즈 정리는 원인과 결과와 같은 한 가지를 다른 것으로 추론하려는 경우 매우 유용하지만, 이 섹션의 뒷부분에서 볼 수 있듯이 역방향의 속성만 알고 있습니다.이 작업을 수행하기 위해 필요한 중요한 작업 중 하나는*소외*입니다.그것은 $P(A, B)$에서 $P(B)$을 결정하는 작업입니다.$B$의 확률은 $A$의 가능한 모든 선택을 설명하고 이들 모두에 대한 공동 확률을 집계하는 데 해당한다는 것을 알 수 있습니다. 

$$P(B) = \sum_{A} P(A, B),$$

이를*sum 규칙*이라고도 합니다.소외의 결과로 나타나는 확률 또는 분포를*주변 확률* 또는*주변 분포*라고 합니다. 

### 독립성

확인해야 할 또 다른 유용한 속성은*의존성* 대 *독립성*입니다.두 개의 확률 변수 $A$ 및 $B$이 독립적이라는 것은 $A$의 한 사건이 발생해도 $B$의 사건 발생에 대한 정보를 나타내지 않는다는 것을 의미합니다.이 경우 $P(B \mid A) = P(B)$입니다.통계학자들은 일반적으로 이를 $A \perp  B$로 표현합니다.베이즈의 정리에서 $P(A \mid B) = P(A)$도 즉시 이어집니다.다른 모든 경우에는 $A$ 및 $B$이라고 부릅니다.예를 들어, 두 개의 연속 다이 롤은 독립적입니다.반대로 조명 스위치의 위치와 실내의 밝기는 그렇지 않습니다 (그러나 항상 전구가 고장 나거나 정전되거나 스위치가 파손될 수 있기 때문에 완벽하게 결정적이지는 않습니다). 

$P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)$는 $P(A, B) = P(A)P(B)$와 동일하므로 두 랜덤 변수는 접합 분포가 개별 분포의 곱인 경우에만 독립적입니다.마찬가지로, 두 랜덤 변수 $A$과 $B$은 $P(A, B \mid C) = P(A \mid C)P(B \mid C)$인 경우에만 다른 랜덤 변수 $C$가 주어진*조건부로 독립적*입니다.이것은 $A \perp B \mid C$로 표현됩니다. 

### 응용 프로그램
:label:`subsec_probability_hiv_app`

우리의 기술을 시험해 보겠습니다.의사가 환자에게 HIV 검사를 시행한다고 가정합니다.이 검사는 상당히 정확하며 환자가 건강하지만 병에 걸린 것으로 보고하는 경우 1% 확률로 실패합니다.또한 환자가 실제로 HIV를 가지고 있다면 HIV를 탐지하는 데 결코 실패하지 않습니다.우리는 진단을 나타 내기 위해 $D_1$를 사용하고 (양성인 경우 $1$, 음성인 경우 $0$) HIV 상태를 나타 내기 위해 $H$을 사용합니다 (양성인 경우 $1$, 음성인 경우 $0$). :numref:`conditional_prob_D1`는 이러한 조건부 확률을 나열합니다. 

:조건부 확률은 $P(D_1 \mid H)$입니다. 

| Conditional probability | $H=1$ | $H=0$ |
|---|---|---|
|$P(D_1 = 1 \mid H)$|            1 |         0.01 |
|$P(D_1 = 0 \mid H)$|            0 |         0.99 |
:label:`conditional_prob_D1`

조건부 확률의 합은 확률과 마찬가지로 1까지 합해야 하므로 열 합은 모두 1이지만 행 합은 그렇지 않습니다.검사가 양성으로 돌아올 경우, 즉 $P(H = 1 \mid D_1 = 1)$가 HIV에 걸릴 확률을 계산해 보겠습니다.분명히 이것은 허위 경보의 수에 영향을 미치기 때문에 질병이 얼마나 흔한지에 달려 있습니다.인구가 매우 건강하다고 가정합니다 (예: $P(H=1) = 0.0015$).베이즈 정리를 적용하려면 소외 및 곱셈 규칙을 적용하여 

$$\begin{aligned}
&P(D_1 = 1) \\
=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\
=& P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \\
=& 0.011485.
\end{aligned}
$$

따라서 우리는 

$$\begin{aligned}
&P(H = 1 \mid D_1 = 1)\\ =& \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} \\ =& 0.1306 \end{aligned}.$$

즉, 매우 정확한 검사를 사용하더라도 환자가 실제로 HIV에 걸릴 확률은 13.06% 에 불과합니다.보시다시피 확률은 직관적이지 않을 수 있습니다. 

그런 끔찍한 소식을 받으면 환자는 어떻게 해야 할까요?아마도 환자는 의사에게 명확성을 얻기 위해 다른 검사를 시행하도록 요청할 것입니다.두 번째 테스트는 특성이 다르며 :numref:`conditional_prob_D2`와 같이 첫 번째 테스트만큼 좋지 않습니다. 

:조건부 확률은 $P(D_2 \mid H)$입니다. 

| Conditional probability | $H=1$ | $H=0$ |
|---|---|---|
|$P(D_2 = 1 \mid H)$|            0.98 |         0.03 |
|$P(D_2 = 0 \mid H)$|            0.02 |         0.97 |
:label:`conditional_prob_D2`

안타깝게도 두 번째 테스트도 긍정적으로 돌아옵니다.조건부 독립성을 가정하여 베이즈 정리를 호출하는 데 필요한 확률을 계산해 보겠습니다. 

$$\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 0) \\
=& P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)  \\
=& 0.0003,
\end{aligned}
$$

$$\begin{aligned}
&P(D_1 = 1, D_2 = 1 \mid H = 1) \\
=& P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)  \\
=& 0.98.
\end{aligned}
$$

이제 소외 및 곱셈 규칙을 적용할 수 있습니다. 

$$\begin{aligned}
&P(D_1 = 1, D_2 = 1) \\
=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\
=& P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\\
=& 0.00176955.
\end{aligned}
$$

결국 환자가 두 가지 양성 검사를 모두 받았을 확률은 다음과 같습니다. 

$$\begin{aligned}
&P(H = 1 \mid D_1 = 1, D_2 = 1)\\
=& \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\
=& 0.8307.
\end{aligned}
$$

즉, 두 번째 테스트를 통해 모든 것이 건강하지는 않다는 확신을 훨씬 높일 수있었습니다.두 번째 테스트가 첫 번째 테스트보다 정확도가 상당히 떨어졌음에도 불구하고 여전히 추정치가 크게 향상되었습니다. 

## 기대와 차이

확률 분포의 주요 특성을 요약하려면 몇 가지 측정이 필요합니다.랜덤 변수 $X$의*기대* (또는 평균) 는 다음과 같이 표시됩니다. 

$$E[X] = \sum_{x} x P(X = x).$$

함수 $f(x)$의 입력값이 서로 다른 값 $x$를 갖는 분포 $P$에서 추출된 확률 변수인 경우 $f(x)$의 기대값은 다음과 같이 계산됩니다. 

$$E_{x \sim P}[f(x)] = \sum_x f(x) P(x).$$

대부분의 경우 랜덤 변수 $X$가 기대치와 얼마나 다른지 측정하려고 합니다.이 값은 분산으로 정량화할 수 있습니다. 

$$\mathrm{Var}[X] = E\left[(X - E[X])^2\right] =
E[X^2] - E[X]^2.$$

제곱근을*표준 편차*라고합니다.랜덤 변수의 여러 값 $x$가 분포에서 샘플링되므로 랜덤 변수 함수의 분산은 함수가 함수의 기대값에서 벗어나는 정도를 측정합니다. 

$$\mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right].$$

## 요약

* 확률 분포에서 표본을 추출할 수 있습니다.
* 관절 분포, 조건부 분포, 베이즈 정리, 소외 및 독립 가정을 사용하여 여러 확률 변수를 분석 할 수 있습니다.
* 기대와 분산은 확률 분포의 주요 특성을 요약하는 데 유용한 측도를 제공합니다.

## 연습문제

1. 우리는 각 그룹이 $n=10$개의 샘플을 추출하는 $m=500$개의 실험 그룹을 수행했습니다.$m$와 $n$은 다양합니다.실험 결과를 관찰하고 분석합니다.
1. 확률이 $P(\mathcal{A})$와 $P(\mathcal{B})$인 두 이벤트가 주어지면 $P(\mathcal{A} \cup \mathcal{B})$ 및 $P(\mathcal{A} \cap \mathcal{B})$에서 상한과 하한을 계산합니다.(힌트: [Venn Diagram](https://en.wikipedia.org/wiki/Venn_diagram)을 사용하여 상황을 표시합니다.)
1. $A$, $B$ 및 $C$과 같은 일련의 랜덤 변수가 있다고 가정합니다. 여기서 $B$은 $A$에만 의존하고 $C$은 $B$에만 종속됩니다. 접합 확률 $P(A, B, C)$를 단순화할 수 있습니까?(힌트: 이것은 [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain)입니다.)
1. :numref:`subsec_probability_hiv_app`에서는 첫 번째 테스트가 더 정확합니다.첫 번째 테스트와 두 번째 테스트를 모두 실행하지 않고 첫 번째 테스트를 두 번 실행하지 않는 이유는 무엇입니까?

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/36)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/37)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/198)
:end_tab:
