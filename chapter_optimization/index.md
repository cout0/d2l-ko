# 최적화 알고리즘
:label:`chap_optimization`

지금까지 책을 순서대로 읽었다면 이미 여러 최적화 알고리즘을 사용하여 딥러닝 모델을 훈련시킨 것입니다.훈련 세트에서 평가한 대로 모델 파라미터를 계속 업데이트하고 손실 함수의 값을 최소화할 수 있는 도구였습니다.실제로, 간단한 설정에서 객관적인 기능을 최소화하기 위해 최적화를 블랙 박스 장치로 취급하는 사람은 그러한 절차의 주문 배열 (“SGD”및 “Adam”과 같은 이름 포함) 이 존재한다는 사실을 알고 만족할 수 있습니다. 

그러나 잘하려면 좀 더 깊은 지식이 필요합니다.최적화 알고리즘은 딥러닝에 중요합니다.한편으로는 복잡한 딥러닝 모델을 훈련시키는 데 몇 시간, 며칠 또는 몇 주가 걸릴 수 있습니다.최적화 알고리즘의 성능은 모델의 훈련 효율성에 직접적인 영향을 미칩니다.반면, 다양한 최적화 알고리즘의 원리와 하이퍼파라미터의 역할을 이해하면 하이퍼파라미터를 목표 방식으로 조정하여 딥러닝 모델의 성능을 개선할 수 있습니다. 

이 장에서는 일반적인 딥러닝 최적화 알고리즘에 대해 자세히 살펴봅니다.딥러닝에서 발생하는 거의 모든 최적화 문제는*비볼록*입니다.그럼에도 불구하고, *볼록* 문제의 맥락에서 알고리즘의 설계 및 분석은 매우 유익한 것으로 입증되었습니다.이러한 이유로 이 장에는 볼록 최적화에 대한 입문서와 볼록 목적 함수에 대한 매우 간단한 확률적 기울기 하강 알고리즘에 대한 증거가 포함되어 있습니다.

```toc
:maxdepth: 2

optimization-intro
convexity
gd
sgd
minibatch-sgd
momentum
adagrad
rmsprop
adadelta
adam
lr-scheduler
```
